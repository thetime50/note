## 6 Brief Introduction of Deep Learning
[ML Lecture 6: Brief Introduction of Deep Learning](https://www.youtube.com/watch?v=Dr-WRlEFefw&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=11)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/DL%20(v2).pdf)

- 1958:Perceptron(linear model)
  类似Logistic Regression，只是没有sigmoid的部分
- 1969:Perceptron has limitation(for MIT)
- 1980s:Multi-later perception
  - 和当今的DNN没有太大的区别
- 1986: Backpropagatino
  - 超过3个隐藏层就很难找到好的结果
- 1989:一个隐藏层就madel任何可能(SDN?)
- 2006:RBM initialization(breakthrough)然并卵
- 2009:GPU 加速
- 2011:用于语音识别
- 2012:赢得ILSVRC

1. define a set of function(Neural Network)
2. goodness of function
3. pick the best function

一个neuron的所有weights 和 biases叫做这neuron的parameter

1 Fully Connect Feedforward Network(全连接前馈神经网络)
![fully-connect-feedforward-network](./img/005-fully-connect-feedforward-network.jpg)  
input vector,output vector

Deep=Many hidden layers(>3 or >8 layers)

### Matrix Operation
![005-matrix-operation](./img/005-matrix-operation.jpg)  
&sigma; is activation function (sigmiod or other)  
Hidden Layers 看作 Feature extractor replacing feature engineering  
Output layers = Multi-class Classifier

### FAQ
- deep model 与非 deep model  
  非deep的model需要做feature engineering  
deep model不需要一个好的feature(选择)直接对数据学习，但是需要design network structure  
  DL在NLP上么没那么好(文本数据相对图像声音数据对电脑较好处理，dl的影响没那么明显)
- How many layers?How many neurons for each layer?  
  Train and Error + Intuition
- 自动学习network structure  
  Evolutionary Artificial Neural Networks人工进化神经网络
- 自己设计network structure  
  convolutional Neural Network(CNN 卷积神经网络)

### Loss of an Example
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;x^1&space;\rightarrow&space;[NN]&space;\rightarrow&space;y^1&space;\underset{C^1}{\Leftrightarrow&space;}\hat&space;y^1" title="x^1 \rightarrow [NN] \rightarrow y^1 \underset{C^1}{\Leftrightarrow }\hat y^1" />  
cross entropy:(输出k维向量)<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;C(y,\hat&space;y)=-\sum_{i=1}^{k}\hat&space;y_i&space;ln&space;y_i" title="C(y,\hat y)=-\sum_{i=1}^{k}\hat y_i ln y_i" />  
Total Loss:<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;L=\sum_{n=1}^N&space;C^n" title="L=\sum_{n=1}^N C^n" />

Find the network parameters &theta;<sup>*</suo> that minimize total loss L  
Gradient Descent....

### Backpropagation

Backpropagation 计算neural network 的<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\partial&space;L/\partial&space;w" title="\partial L/\partial w" />