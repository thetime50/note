## 15 Unsupervised Learning - Neighbor Embedding

[15 Unsupervised Learning - Neighbor Embedding](https://www.youtube.com/watch?v=GBUEjkpoxXc&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=24)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/TSNE.pdf)  

t-SNE: t-Distributed Stochastic Neighbor Embedding  分布式随机邻近嵌入(映射) 可以通过 Barnes-Hut 近似来实现


[什么是 embedding? - 知乎](https://www.zhihu.com/question/32275069/answer/80188672)
Embedding在数学上表示一个maping, f: X -> Y， 也就是一个function  
其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）  
structure-preserving (结构保存，比如在X所属的空间上X1 < X2,那么映射后在Y所属空间上同理 Y1 < Y2)。  
(单射和保持结构)

## Manifold learning

近距离时可以使用 Euclidean distance,然后把高维数据展开为低纬  
降维后可以直接用 Eculidean distance 计算距离  
对clustering / supervised learning 有所帮助


## Locally Linear Embedding (LLE)

对于两个点 x<sup>i</sup> x<sup>j</sup> 找到两点之间的关系 w<sub>ij</sub>  
一个点等于相邻点的线性组合  
minimizing:  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;w_{ij}=&space;\sum_i&space;\left\|&space;x^i&space;-&space;\sum&space;_j&space;w_{ij}&space;x^i&space;\right\|_2" title="\bg_white w_{ij}= \sum_i \left\| x^i - \sum _j w_{ij} x^i \right\|_2" />

用相邻点 <img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\sum&space;_j&space;w_{ij}&space;x^i" title="\bg_white \sum _j w_{ij} x^i " /> 组合为 x<sup>i</sup>

将原本的 x<sup>i</sup> x<sup>j</sup> 通过w<sub>ij</sub>变换为 x<sup>j</sup> z<sup>j</sup>  
z<sup>i</sup> z<sup>j</sup>  的维度比原来 x<sup>i</sup> x<sup>j</sup> 的维度要少  

neighbor 个数需要手动调一下

这个 w 是怎么转换x vector   的? 转换后有几个 dimension?


## Laplacian Embedding

Graph-based approach  
smoothness assumption: 计算两点间距离只使用Euclidean distance是不够的，还需要在 high density 中的区域之间的距离，可以把datapoint construct成一个图，两点的相似度到一个 threshold 就连接起来

<br>

在说 semi-supervised learning 时：

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;L=&space;\sum_{x^r}C(y^r,\hat&space;y^r)&space;&plus;&space;\lambda&space;S" title="\bg_white L= \sum_{x^r}C(y^r,\hat y^r) + \lambda S" />

&lambda;S像 regularization 的项??

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;S=&space;\frac{1}{2}\sum_{i,j}(y^i-y^j)^2&space;=&space;y^TLy" title="\bg_white S= \frac{1}{2}\sum_{i,j}(y^i-y^j)^2 = y^TLy" />

w<sub>i,j</sub>是相似程度，如果不相连为0  
S 就是评价现在得到的label有多么smooth

L: (R+U) x (R+U) matrix 就是graph的laplacian  
L=D-W

这也同样用在 unsupervised learning 中

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;S=&space;\frac{1}{2}\sum_{i,j}(z^i-z^j)^2&space;" title="\bg_white S= \frac{1}{2}\sum_{i,j}(z^i-z^j)^2 " />

后面应该使用 Ueculidean distance 或者 tow norm distance 比较好

需要给z一些限制，避免z为0的情况。如果z的维度是M ,那么 Span{z<sup>1</sup>,z<sup>2</sup>,...z<sup>N</sup>, }=R<sup>M</sup>

z就是 graph laplacian matrix 比较小的 eigenvalue 的 eigenvector  

先找出z 再用 K-means 做 clustering 的话就叫 spectral clustering 


## t-SNE

上面只约束了相近的点要接近，但是没有约束不相近的点要分开。 所以会遇到k值太小或者维度过低时 embedding 后数据会重叠在一起。

LLE on mnist  
LLE on COIL-20

记任意两点的相似(similarity)为 S(x<sup>i</sup>,x<sup>j</sup>)  
normalization 归一化  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;P(x^j|x^i)&space;=&space;\frac{S(x^i,x^j)}{\sum_{k\neq&space;i}S(x^i,x^j)}" title="\bg_white P(x^j|x^i) = \frac{S(x^i,x^j)}{\sum_{k\neq i}S(x^i,x^j)}" />  
两点距离除以其他所有距离之和

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;P(z^j|z^i)&space;=&space;\frac{S(z^i,z^j)}{\sum_{k\neq&space;i}S(z^i,z^j)}" title="\bg_white P(x^j|x^i) = \frac{S(x^i,x^j)}{\sum_{k\neq i}S(x^i,x^j)}" />  

KL divergence
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;L=\sum_i&space;KL(P(*|x^i)||Q(*|z^i|))=\sum_i\sum_j&space;P(x^j|x^i)log(\frac{P(x^j|x^i)}{P(z^j|z^i)})" title="\bg_white L=\sum_i KL(P(*|x^i)||Q(*|z^i|))=\sum_i\sum_j P(x^j|x^i)log(\frac{P(x^j|x^i)}{P(z^j|z^i)})" />  
对z做微分  


会计算所有data point 的 similarity 计算量比较大,比如数据量多或者原始数据维度比较大  
先PCA降维再做t-SNE

对于新数据 t-SNE要重新计算所有的similarity 所以一般不会在用在 training tetsting 的状情况下  
一般是用来做Visulization  


SNE (RBF function): <img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;S(x^i,x^j)&space;=&space;exp(-\left\|&space;x^i-x^j&space;\right\|_2)" title="\bg_white S(x^i,x^j) = exp(-\left\| x^i-x^j \right\|_2)" />  
t-SNE: <img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;S(x^i,x^j)&space;=&space;1/1&space;&plus;&space;\left\|&space;x^i-x^j&space;\right\|_2" title="\bg_white S(x^i,x^j) = 1/1 + \left\| x^i-x^j \right\|_2" /> 这是 t-distribution 的一种  
t-SNE 会让较远的距离呈指数放大


