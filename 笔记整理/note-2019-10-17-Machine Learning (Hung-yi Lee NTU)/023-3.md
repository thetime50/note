## 23-3 Reinforcement Learning (including Q-learning) 

与一般Machine Learning 不同的是机器输出的Action会影响到下一次观察到的环境状态  

alpha go


外界输入的资讯没有处理的叫 observation，给他归纳处理后叫做state  
但是机器处理能力越来越强，可以不用单独做归纳直接输入    

behavior cloning 复制行为

用supervised learning 的逻辑学习一个模型会有什么样的问题呢？  
会模仿无关结果的动作和错误的动作  
没法区分什么样的行为实际上是对结果有正面影响的  

要把所有的action 序列当作整体来看待  
解决方法：  
1. Reinforcement Learning // 强化学习  
2. learning by demonstration (inverse reinforcement learning ) // 演示学习  
  有一些专家行为的demo,通过特别的学习方式  


- Actor
- Critic
- Actor+Critic

### Reinforcement Learning

强化学习的3个结构
- Actor
- Environment // 固定的
- Reword function // 固定的 

每一场游戏从头到尾叫一个Episode  
把 s<sub>1</sub>,a<sub>1</sub>,s<sub>2</sub>,a<sub>2</sub>的序列统统记录起来， 叫一个 Trajectory  
把每一个 reward t<sub>t</sub>加 起来加 total reward R(&tau;)

过去的actor 是查表的，现在是 neural network  
(如何解决 non-linear 的问题)

如果要train 的问题中有未知的 不能微分的部分，就用 policy gradient  来一发  

### Critic


