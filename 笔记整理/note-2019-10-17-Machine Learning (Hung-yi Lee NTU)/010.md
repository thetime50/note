## 10 Convolutional Neural Network
[10 Convolutional Neural Network](https://www.youtube.com/watch?v=FrKWiRv254g&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=19)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/CNN.pdf)


supervised learning的相关建议

### Why CNN for Image
Neural Network里边每一个layer都是一个classifier，每一个neural都是特征识别器提取特征

CNN中使用prior knowledge去掉fully connected layer里面的参数 减少数据量

如何减少参数  
- property
    1. 第一层用图片上的一小部分提取pattern(范围)
    2. pattern会出现在图上的不同位置，但是使用相同的一  参数检测(位置)
    3. Subsampling 取出奇数行偶数列

pattern的局部范围和位置问题通过Convolution来处理
Subsampling 使用Max Pooling来处理

- steps
    1. image
    2. Convolution(卷积 局部特征提取强化的过程)
    3. Max Pooling(池化)
    4. (loop step 2 multiple)
    5. Flatten
    6. Fully Connected Feedforward network

### Convolution
Convolution内有一组filter 一个filter内包含一块区域的parameter(即局部的neuron) (filter is Matrix)  
一个filter依次与图片上局部像素点做内积，每做一次计算(内积)移动stride个距离。一层有j个filter，每个filter可以做k\*k次内积 (卷积的区域)，最后得到深度为j的k*k的Matrix，共j\*k\*k个计算结果。即Feature Map。

(cnn可以前接network输出scalar 做旋转缩放等预处理 -deepMind)

如果是rgb的图片，Filter是一个深度为3的cube。i\*i\*3的像素和i\*i\*3的filter cube做内积。(还是一次内积得到一个数据点，最后的数据量不变j\*k\*k)深度又叫channel。  
输入的时候rgb 3个颜色所以说channel,也是deep为3。中间Convolution的输出j个filter，那么就有j个channel，deep也为j。一个filter会包括前级分区内的所有channel.

filter相当于fully connect layer在其他位置的weight为0，并且对图上的每个位置都用同一个neuron提取特征，weight sharing

cube&cdot;cube  cube is all channel in 1 value

**Convolution 怎么train**
同样做 backpropagation ,得到每个filter在每个stride区域里的 gradient , 然后把同个filter的gradient做平均

### Max Pooling
划分一定区域取最大值或做平均(微分方法参考Maxout Network)  
做完Max Pooling后Matrix的尺寸变小 深度不变  
(相当于取池化区域内最大的特征)

### Flatten
最后展开Feater Map为j\*k\*k的vector集合  
然后给Folly connect feedforward network处理


### CNN in Keras
input format 为3-D tensor的数据(rgb)或2D-tensor(黑白)  
加入Convotion层

(Marix is 2D tensor,Cube is 3D tensor)

```python
model.add(Convolution(25,3,3, #filter cnt/w/h
    input_shape=(1,28,28))) #input channel,w/h

model.add(MaxPolling2D((2,2))) #w/h

model.add(Convolution(50,3,3)) #filter cnt/w/h

model.add(MaxPolling2D((2,2))) #w/h

model.add(Flatten()) #Flatten??

model.add(Dense(outoyt_dim=100))
model.add(Activation('rely'))
model.add(Dense(output_dim=10))
model.add(Activation('shoftmax'))
```

每一个filter(neuron)有fh\*fw\*channel个参数

filter是一个神经元节点能够提取出一个特征，channel 是一组输入数据输出的结果的特征种类

### What does CNN learn

如何知道一个filter在做什么

- 拿出第k个filter的数据<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;[a_{ij}^k]" title="[a_{ij}^k]" />  
- Degree of the activation of the k-th filter:<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;a^k=\sum_{i=1}^{11}\sum_{j=1}^{11}a_{ij}^k" title="a^k=\sum_{i=1}^{11}\sum_{j=1}^{11}a_{ij}^k" />
- input x <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;x^*=arg&space;\max_x&space;a^k" title="x^*=arg \max_x a^k" />(gradient ascent)

知道一个neuron在做什么

第j个neuron的输出为a<sup>j</sup> 
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;x^*=arg&space;\max_x&space;a^j" title="x^*=arg \max_x a^j" />


第i个output在做什么  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;x^*=arg&space;\max_x&space;a^i" title="x^*=arg \max_x a^i" />  

输出节点经过处理后画面都是噪点，所以添加规则  
regularization constraint 有数据的地方少(L<sub>0</sub>Regularization)
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;x^*=arg&space;\max_x&space;(y^i-\sum_{i,j}|x_{ij}|)" title="x^*=arg \max_x (y^i-\sum_{i,j}|x_{ij}|)" />  
或者相邻piciv同色等

https://www.youtube.com/watch?v=M2IebCN9Ht4

### Deep Dream
img&rightarrow;CNN 取一个filter或者hidden layer计算结果拿出来(vector)  
把输出的vector里边positive的dimension调大 negative的dimension调小  
gradient descent找到匹配的图  
(找一张匹配特征更突出的图)  
http://deepdreamgenerator.com

### Deep Style
https://arxiv.org/abs/150  
8.06576

img1&rightarrow;CNN&rightarrow;content  
img2&rightarrow;CNN&rightarrow;style(filter 之间的correlation)  

(filter 的out接近img2 最终的out接近img1 ??)

### More Application
**围棋**

AlpasGo Supervised 部分CNN,Reinforcement learing部分

什么时候能用CNN
- patterns 适用于小区域的规则  
  AlphaGo uses 5*5 filter for first layer

围棋不需要MaxPooling层  
input is 19*19*48,每个位置48种状态 domain knowledge  
zero pads  
5*5 filter for first layer,128/256个filter,stride 1  
Activation function ReLU  
2~12 layers  
3*3 filter


CNN 使用人掌握的知识观察的规则减小学习的数据量。（从上级逻辑开始限制，细节交给机器）

**speech**
Spectrogram&rightarrow;image  
&rightarrow;phoneme

在识别phoneme的时候只在frequency方向上移动filter  
frequency方向上的特征是可复用的，<del>水平方向上可能可以删除 但是特征不能复用</del>  
语音识别后面还会接LSTM等，会考虑typical的information

根据应用的特性设计Network的structure

**text**
word sequence，word embedding组成  
filter沿时间方向移动(时间方向上的特征是可复用的)

**画出数字**

- PixelRNN https://arxiv.org/abs/1601.06759
- Variation Autoencoder (VAE) https://arxiv.org/abs/1312.6114
- Generative Adversarial Networks (GAN) https://arxiv.org/abs/1406.2661
