## 16 Unsupervised Learning - Auto-encoder

[16 Unsupervised Learning - Auto-encoder](https://www.youtube.com/watch?v=Tk5B4seA-AU&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=25)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/auto.pdf)  

img = NN Encoder => code (维度更少)// 特征提取 压缩信息  
code = NN Decoder => img  
code 又是buttleneck layer

PCA: img vector  =W matrix=> c =W<sup>T</sup>=> x&#770;

### Auto encoder be deep

![016-auto-encode](./img/016-auto-encode.jpg)

最早要使用RBM做layer wise 的initialization才行，不然会tranin坏掉。  
可以让 buttleneck layer 两边的weight是镜像的，互为 transpose。但不是必要的。

同样使用PCA 和deep autoencoder 处理MNIST 到2维，PCA 的数据是混在一起的，而deep autoencoder可以把数据分开。


**用在文字处理上**
用 Vector space Model 做文字搜寻 

把每一个文章用vector表示,搜索单词也表示为一个vector。然后做 inner product 或者 cosine similarity

如何把一的document 表示为一个 Vector呢
1. bag-of-word 再在单词上乘weight, 这养没法结合上下文的语义
2. 使用deep autoencoder 将文章处理为低维  
    和使用LSA(latent semantic analysis潜在语义分析)相比，LSA处理后是混在一起的

**用在图像搜索上 以图找图**

**用在 pretraining 上**  
做参数的 initialization

例如做 MNIST 的时候，输入784 dimension 数据最终输出10维数据，模型结构为  
imput 784 -> 1000 -> 1000 -> 500 -> 10  
可以先使用 auto encoder 做 784 -> 1000 -> 784, 这里encoder维度大于输入维度时要注意避免learn一个类似 identity matrix 出来，所以要加一个 L1 的 regularization ，让矩阵是 sparse 的,输出只有某几维有值其他都是0。
载用第二层的1000维数据 做 1000 -> 1000(3) -> 10000 的 auto encoder  
再用第三层的1000维数据 做 1000 -> 500(4) -> 10000 的 auto encoder  
再用 random initilization 做 500 -> 10 的 weight
这样做 network 的 初始化,  
再重新 back propagation 整个网络 fine tune  
现在可以不使用pretraining的技术也能够train起来  
但是可以先用大量的unlabeled data先learn好initialize模型参数，就可以使用少量labeled data完成最后的微调训练。  

### De-noising Autoencoder
autoencoder 去噪声  
input x =add noise=> x' =encode=> c =decode=> y  
把最终输出y 和加入噪声前的原始输入x 做 diff 来训练model

**Contractive auto-encoder**

learn model 时加上一些限制，输入有noise时但是对中间code影响不大

- Restricted Boltzmann Machine (RBM) // 不是 neural network 是 graph co-model
- deep belief network (DBN) // 是 graph co-model

### Auto-encoder for CNN
对图像 autoencoder
```
      Convolution => Pooling => Convolution => Pooling  
                                                  ||
                                                  \/
Deconvolution <= Unpooling <= Deconvolution <= Unpooling  
```

- Unpooling  
    放大还原图片时，只在取样的位置放置数据，其他位置补0  
    或者直接复制数据
- Deconvolution：相当于边界补零放大的 convolution (weight顺序是反的有关系吗)

**sequence-to-sequence Auto-encode**  
与RNN有关

**使用decode生成image**
可以用decode产生新的image  
如何定位code是在能够生成的图片的区域里？ train时加 L2 regularization 让code分布接近0






