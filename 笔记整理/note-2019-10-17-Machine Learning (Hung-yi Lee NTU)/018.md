## 18 Unsupervised Learning - Deep Generative Model (Part II)

[18 Unsupervised Learning - Deep Generative Model (Part II)](https://www.youtube.com/watch?v=8zomhgKrsmQ&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=27)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/GAN%20(v3).pdf)

![vae](./img/017-vae.jpg)

- m 是实际的encoder输出
- &sigma; 是方差 variance ?? 自动learn出来的?? 
- e 是 mormal distribution sample的值 随机的权重

相当于把 sample data encode附近加上一些noice 扩散。

在minimiae上有 exp(&sigma;<sub>i</sub>)-(1+&sigma;<sub>i</sub>) 项,此项最小为0。  
所以对于exp(&sigma;<sub>i</sub>)最小为1，保证noice不为。  
(m<sub>i</sub>)<sup>2</sup>项是加了 code 的 L2 regularization 让结果 sparse。 


把一笔data point x 模拟到周围附近的概率分布 P(x)


**Gaussian Mixture Model**  
多个高斯分布不同weight累加起来  

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;P(x)=\sum_m&space;P(m)P(x|m)" title="\bg_white P(x)=\sum_m P(m)P(x|m)" />

x|m ~ N(&mu;<sup>m</sup>, &sigma;<sup>m</sup>)  
决定mixture的数目  
在 Gaussian Mixture Model 中，通过一把data x来估计这些 Gaussian 的weight和mean和variance。 使用 EM Algorithm 

VAE 就是 Gaussian Mixture Model 的 distribution representation版本。  
<s>每一个 Gaussian 就是一个 feature。被VAE投影叠加在低维空间上。</s>  
VAE 是通过 auto-encoder 来learn 一个 Gaussian Mixture Model，  
对于前半部分的Encoder学会通过将input 生成 对应的 mearn 和 variance，而一个NN Encoder 要满足所有的input data,会让所有的input 的 mern 和 variance 有所联系。  
而后面的Encoder 对于一个输入的 mearn 组合应该会有怎样对应的 variance，减小一些 dimension 的 weight。  
testing data 时候相当于 e = [0] ,即只取mearn概率最大的情况

如果输出只有一个像素的话比较好理解， 有点像一般的 neural network。  
但是对于多维的输出 是不是每个输出都要有一组 mearn 和 variance？(当然这样像素就变成独立的了)

variance 是一个diagonal matrix ，可以只输出对角的数值,(可以不是 diagonal 的吗?)


// todo 15"40'


### conditional VAE

提取图片特性，把特性的distribution 和指定数字告诉Encoder。就能够产生相同特性的图片   

https://arxiv.org/pdf/1406.5298v2.pdf


VAE的问题并没有去学怎么生成像真的图片， 这是拟合了pixel的位置，如果在图像上加一个pixel 对model 来说是一样的。

### Generative Adversarial Nerwork (GAN)

NN Generatot => generate img  
(generate img  + data img ) => NN Discriminator => is real or fake  
循环进化

- NN Discriminator 输入img,输出0-1的可信度
- NN Generate 类似VAE decoder 的架构(code 怎么产生？),input 是从一个 distribution sample 出来的 vector

generate image label is 0  
real image label is 1  
binary classification problem

怎么update Generater?  
其实 generate 就是一个 hidden layer， generate + discriminator 是一整个 neural network

Gan很难train


不知道 Discriminator 对不对，Discriminator 效果好可能是 Generate 太废了。Discriminator 效果不好可能是 Generate 太好了，也可能是 Discriminator 自己没train好,  
也有可能是Generate 不管输入什么都只输出同一张很像的图片，(是不是可以加L2 regularization)

https://openai.com/blog/generative-models/

可以生成风格内容渐变过度的图像
