# 20 Support Vector Machine (SVM)

[20 Support Vector Machine (SVM)](https://www.youtube.com/watch?v=QSEPStBgwRQ&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49)  
[pdf](https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/SVM%20(v5).pdf)  


[什么是 SVM（支持向量机）？ KnowingAI知智](https://www.bilibili.com/video/av373770998?from=search&seid=4130925215194615253&spm_id_from=333.337.0.0)

Hinge Loss + Kernel Method => Sepport Vector Machine (SVM)

(Kernel Trick 核技巧 方法)

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\\L(f)&space;=&space;\sum_n&space;\sigma(g(x^n)\neq&space;\hat&space;y^n)&space;&space;\Rightarrow&space;\\L(f)&space;=&space;\sum_n&space;l(f(x^n),&space;\hat&space;y^n)" title="\bg_white \\L(f) = \sum_n \sigma(g(x^n)\neq \hat y^n) \Rightarrow \\L(f) = \sum_n l(f(x^n), \hat y^n)" />

g(x) = \[-1,1\]  
f(x) 是警司函数  
&sigma;(g(x),y&#770;) = \[if f(x)==y&#770; is 0 ; else is 1\]  
l(f(x),y&#770;) 是近似方法  

1. square loss  
l(f(x),y&#770;) = (y&#770;f(x)-1)<sup>2</sup> => square loss 方差损失函数  
不合理，同binary classification 的loss 问题一样，距离分割线较远的数据会有很大的loss

2. sigmoid + square loss  
l(f(x),y&#770;) = (&sigma;(y&#770;f(x))-1)<sup>2</sup> // &sigma;() 表示 sigmoid function  
在极大和极小的时候 gradient 很小

3. cross entropy  
l(f(x),y&#770;) = ln(1-exp(-y&#770;f(x)))

---
<br>
**hinge loss**: 
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;l(f(x^n),\hat{y}^n)=max(0,1-\hat{y}^n&space;f(x))" title="\bg_white l(f(x^n),\hat{y}^n)=max(0,1-\hat{y}^n f(x))" />

if positive example y&#770; = 1: max(0,1-f(x)) , f(x)>1 
if negative example y&#770; = -1: max(0,1+f(x)) , f(x)<-1 

like one-shot learning

hinge loss 对比 cross entropy 比较不害怕 outlier (离群值)


### linear SVM

step 1: function model is <u>linear </u>  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;f(x)=\sum_i{w_i&space;&space;x_i&space;&plus;&space;b}=\begin{bmatrix}w&space;\\b\end{bmatrix}\cdot\begin{bmatrix}w&space;\\b\end{bmatrix}&space;" title="\bg_white f(x)=\sum_i{w_i x_i + b}=\begin{bmatrix}w \\b\end{bmatrix}\cdot\begin{bmatrix}w \\b\end{bmatrix} " />

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\begin{bmatrix}w&space;\\b\end{bmatrix}" title="\bg_white \begin{bmatrix}w \\b\end{bmatrix}" /> is new **w**  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\begin{bmatrix}x&space;\\1\end{bmatrix}" title="\bg_white \begin{bmatrix}x \\1\end{bmatrix}" /> is new **x**

step 2: loss is hinge loss + regularization term

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;L(f)=&space;\sum_n&space;l(f(x^n),\hat{y}^n)&plus;\lambda||w||_2" title="\bg_white L(f)= \sum_n l(f(x^n),\hat{y}^n)+\lambda||w||_2" />  
是个convex function,每一项都是convex function 

step 3: optimization gradient 或者 


linear SVM 与 logistic regression 的区别基就只有 Loss function  不同  
linear SVM Hinge Loss  
logistic Cross Entrropu  

SVS 的 Function (Model) 也可以不是linear 的， 可以是deep 的


### optimization
#### 用 Gradient Descent
Gradient Descent train SVM 的方法 叫做 Picasso

![020-svm-gradient](./img/020-svm-gradient.jpg)

#### 用 //

用 notation epsilon n 替代 <img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;l(f(x^n),\hat{y}^n)" title="\bg_white l(f(x^n),\hat{y}^n)" />  

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\varepsilon^n&space;=&space;max(0,1-\hat{y}^n&space;f(x))&space;" title="\bg_white \varepsilon^n = max(0,1-\hat{y}^n f(x)) " />

在 **Minimizing loss function L** 的条件下 会等于

![020-svm-optimization](./img/020-svm-optimization.jpg)

让 y&#770;<sup>n</sup>f(x) > 1,但是有时候这个条件满足不了，用 &epsilon;<sup>n</sup>来放宽这个条件  
&epsilon;<sup>n</sup> 被叫做 slack variable 松弛变量  
&epsilon;<sup>n</sup> 必须 >=0


### 二元表示

更关注边界附近的数据

用上面 minimize loss function 找出来的weight 就是 data 的 linear combination  
就是对 data 的所有数据 做 wx+b 的操作，每一笔数据都有各自的 w 和 b  
可以用 Lagrange multiplier解出证明  

(1) <img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;w^*&space;=&space;\sum_n\alpha_n^*x^n" title="\bg_white w^* = \sum_n\alpha_n^*x^n" /> 

就是对每一笔数据线性操作后累加  

可以算出 Gradient Descent 的式子为：  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;w_i&space;\leftarrow&space;&space;w_i-\eta&space;\sum_nc^n(w)x_i^n" title="\bg_white w_i \leftarrow w_i-\eta \sum_nc^n(w)x_i^n" />


<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;c^n(w)=\frac{\partial&space;l(f(x^n),\hat{y}^n)}{\partial&space;f(x^n)}" title="\bg_white c^n(w)=\frac{\partial l(f(x^n),\hat{y}^n)}{\partial f(x^n)}" />

在使用hinge loss 时，如果他作用在max=0的region的话 c<sup>n</sup>(w) 就会是0，会有很多数据对 gradient update 是没影响的  
最后解出来的 Linear Coombination 的 weight 可能会是 sparse 的  
很多的data point 对应 (1)式中的 &alpha;<sup>*</sup> 是0  
&alpha;<sup>*</sup> 值不等于0的data point 就叫做support vector  

所以 svm 相对于其他方法(如 cross entropy) 更 robust。对 outlier 不敏感  
不是 support vector 的 data point 拿掉对模型一点影响都没有  


把w 写成 data point 的 lineat combination 方便使用 Kernel trick

### Kernel Method

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;w=\sum_n\alpha_nx^n&space;=&space;X&space;\alpha" title="\bg_white w=\sum_n\alpha_nx^n = X \alpha" />

带入 step 1 function model  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;f(x)&space;=&space;w^Tx&space;=&space;\alpha^T&space;X^T&space;x&space;=&space;\sum_n&space;\alpha_n(x^n\cdot&space;x)" title="\bg_white f(x) = w^Tx = \alpha^T X^T x = \sum_n \alpha_n(x^n\cdot x)" />


定义 kernel function <img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;K(x^n,x)&space;=&space;(x^n&space;\cdot&space;x)" title="\bg_white K(x^n,x) = (x^n \cdot x)" />

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;f(x)&space;=&space;\sum_n\alpha_n&space;K(x^n,x)" title="\bg_white f(x) = \sum_n\alpha_n K(x^n,x)" />

step 2,3 find <img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\{\alpha_1^n&space;...,&space;\alpha_n^*,...&space;\alpha_N&space;^*\}" title="\bg_white \{\alpha_1^n ..., \alpha_n^*,... \alpha_N ^*\}" /> minimization loss function L

![020-kernel-trick.jpg](./img/020-kernel-trick.jpg) 

kernel-trick 不只能用在SVM 里面， Linear Regression 和 Logistic Regression 都可以用

### Kernel Trick

优化求解运算的方法  

先将x 做 feature transform 为 &Phi;(x)

对一笔x 先做 feature transform 再去 apply Linear SVM 

对一笔x 先做 feature transform 再去 做inner product 等同于先做 inner product 再平方

直接计算 x 和 z inner product 后带近 Kernel Function 以后的 output 比先做 Feature Transform 再做 inner product 更快


### Radial Basis Function Kernel

// todo

展开后会变成无穷多维  
容易 overfitting  

### sigmoid kernel

// todo 

只要定义一个Function 可以 evaluate x 和 Z 的 similarity 就可以当作 kernel来用。并且要能够表示为 两个vector 做 inner product 的结果    

Mercre's theory 可以确定什么 function 可以作为 kernel function。

### SVM related methods

就是针对不同分布形状的支持向量模型

- Support Vector Regression： 带状分布
- Ranking SVM: 排序问题
- One-class SVM: 单类问题 集中在一个区域


### deep learning 和 SVM 的差别

deep learnint 前几个layer 做feature transform,最后的layer 做classifier  
SVM 先apply 以恶 Kernel Function 把feature 转换到一个 height dimension 上面，然后用 Hinge Loss apply linear classifier  

SVM 的 kernel 是 learnable 的，可学习的  

可以有好几个不同的 kernel 把不同 kernel 加上weight combine起来，这weight 是可以learn 的  

如果只有一个kernel 的时候，SVM就像是只有一个 hidden layer 的 neural network  
当把kernel 做linear combination 的时候就像有两个hidden layer 的 neural network  
