## 3-1 Gradient Descent
[ML Lecture 3-1: Gradient Descent](https://www.youtube.com/watch?v=yKKNr-QKz2Q&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=6)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Gradient%20Descent%20(v2).pdf)

### Review：Gradient Descent
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\theta^*=arg&space;\min_\theta{L(\theta)}" title="\theta^*=arg \min_\theta{L(\theta)}" />  
L:loss function  
&theta;:parameters

Suppose that &theta; has two Variables {&theta;<sub>1</sub>,&theta;<sub>2</sub>}

Randomly start an <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\theta^0=\begin{bmatrix}&space;\theta^0_1\\&space;\theta^0_2&space;\end{bmatrix}" title="\theta^0=\begin{bmatrix} \theta^0_1\\ \theta^0_2 \end{bmatrix}" />

The Gradient Variable is:<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\nabla&space;L(\theta)=&space;\begin{bmatrix}&space;\partial&space;L(\theta_1)/\partial&space;\theta_1\\&space;\partial&space;L(\theta_2)/\partial&space;\theta_2&space;\end{bmatrix}" title="\nabla L(\theta)= \begin{bmatrix} \partial L(\theta_1)/\partial \theta_1\\ \partial L(\theta_2)/\partial \theta_2 \end{bmatrix}" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\begin{bmatrix}&space;\theta_1^1\\&space;\theta_2^1&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;\theta_1^0\\&space;\theta_2^0&space;\end{bmatrix}&space;-\eta&space;\begin{bmatrix}&space;\partial&space;L(\theta_1^0)/\partial\theta_1\\&space;\partial&space;L(\theta_2^0)/\partial\theta_2&space;\end{bmatrix}" title="\begin{bmatrix} \theta_1^1\\ \theta_2^1 \end{bmatrix} = \begin{bmatrix} \theta_1^0\\ \theta_2^0 \end{bmatrix} -\eta \begin{bmatrix} \partial L(\theta_1^0)/\partial\theta_1\\ \partial L(\theta_2^0)/\partial\theta_2 \end{bmatrix}" />
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\Rightarrow&space;\theta^1=\theta^0-\eta\nabla&space;L(\theta^0)" title="\Rightarrow \theta^1=\theta^0-\eta\nabla L(\theta^0)" />

Gradient 是等高线法线方向

### Adaptive Learning Rates

1. 随着学习次数增加，Leaning Rate 减小，学习速率/参数变化降低
  decay:<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\eta^t={\eta}/{\sqrt{t&plus;1}}" title="\eta^t={\eta}/{\sqrt{t+1}}" />

### Adagrad
(1) 每个参数不同的Leaning Rate
(2) 分别除以先前的均方差(root mean square)

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\eta^t=\frac{\eta}{\sqrt{t&plus;1}}" title="\eta^t=\frac{\eta}{\sqrt{t+1}}" /> &eta;<sup></sup>是引入了衰减的学习率  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;g^t=\frac{\partial&space;C(\theta^t)}{\partial&space;w}" title="g^t=\frac{\partial C(\theta^t)}{\partial w}" />  (C是Loss function??) 当前参数对损失函数的偏微分  
*w is one parameters*  
*todo &theta; 就是w???*

**Vanilla Gradient descent**  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;w^{t&plus;1}\leftarrow&space;w^t-\eta^tg^t" title="w^{t+1}\leftarrow w^t-\eta^tg^t" />

**Adagrad**  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;w^{t&plus;1}\leftarrow&space;w^t-\frac{\eta^t}{\sigma^t}g^t" title="w^{t+1}\leftarrow w^t-\frac{\eta^t}{\sigma^t}g^t" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\sigma^t=\sqrt{\frac{1}{t&plus;1}\sum_{i=1}^n&space;(g^i)^2}" title="\sigma^t=\sqrt{\frac{1}{t+1}\sum_{i=1}^n (g^i)^2}" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\because&space;\eta^t=\frac{\eta}{\sqrt{t&plus;1}}" title="\because&space;\eta^t=\frac{\eta}{\sqrt{t+1}}" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\therefore&space;w^{(t&plus;1)}\leftarrow&space;w^t-\frac{\eta}{\sqrt{\sum_{i=0}^t(g^i)^2}}g^t" title="\therefore w^{(t+1)}\leftarrow w^t-\frac{\eta}{\sqrt{\sum_{i=0}^t(g^i)^2}}g^t" />

*Adam Grad*较稳定

Adagrad 中 &sigma;项倾向于让学习的参数保持稳定

在二次函数<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;y=ax^2&plus;bx&plus;c" title="y=ax^2+bx+c" />中  
极值点位置处于<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;-\frac{b}{2a}" title="-\frac{b}{2a}" />  
对于图像上任意位置的x<sub>0</sub>到极值点的步长最优解为  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;|x_0&plus;\frac{b}{2a}|=|\frac{2ax&plus;b}{2a}|" title="|x_0+\frac{b}{2a}|=|\frac{2ax+b}{2a}|" />  
其中 |2ax<sub>0</sub>+b| 是函数的一次微分项  
分母 2a 是函数和二次微分项

所以在Gradient Descent中一次微分项作为分母(g<sup>t</sup>)，二次微分项在分子的位置(即&sigma;<sup>t</sup>的作用)

即当前微分值越大(Gradient Variable越大)则离最低点越远 进步的步长也越大  
当二次微分值越大，说明一次微分变化率快，当前位置距离最低点近，进步步长减小。

### Stochastic Gradient Descent

1) Gradient Descent  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;L=\sum_n(\hat&space;y^n-(b&plus;\sum&space;w_i&space;x_i^n))^2" title="L=\sum_n(\hat y^n-(b+\sum w_i x_i^n))^2" />  
(这里的Loss是验证了所有example的标准差之和)  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\theta^i=\theta^{i-1}-\eta\nabla&space;L(\theta^{n-1})" title="\theta^i=\theta^{i-1}-\eta\nabla L(\theta^{n-1})" />  
每轮更新参数使用所有数据一个Loss更新一轮参数

2) Stochastic Gradient descent  
对每个example使用独立的L<sup>n</sup>(loss function),一个样本一个loss  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;L^n=(\hat&space;y^n-(b&plus;\sum&space;w_ix_i^n))^2" title="L^n=(\hat y^n-(b+\sum w_ix_i^n))^2" />  
每轮更新对每个example使用独立的Loss(独立的)更新一次参数，一轮更新中有n个样本就会有n次参数更新

每个一次的数据点运算都在修正模型所以速度能够赶快，而不是等到一轮运算结束后才更新一次参数

### Featurn Scaling
特征缩放

让所有输入特征的分布范围大致相同，(这样改变不同特征的参数对输出和error的影响也会比较一致，在学习起始阶段也能推进数值小的特征的参数的学习，等高线的法线也会比较直 减少学习步骤)

for each dimension i(有i维的数据):  
mean: m<sub>i</sub>
standard deviation:&sigma;<sub>i</sub>

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;x_i^r\leftarrow\frac{x^r_i-m_i}{\sigma_i}" title="x_i^r\leftarrow\frac{x^r_i-m_i}{\sigma_i}" />

The means of all dimensions are 0,  
and the variances are all 1

### Gradient Descent Theory
nothing

### Warning of Math
**Taylor Series**  
*todo 阶分??*

对于x=x<sub>0</sub>  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;h(x)=\sum_{k=0}^{\infty}\frac{h^{(k)}(x_0)}{k!}(x-x_0)^k" title="h(x)=\sum_{k=0}^{\infty}\frac{h^{(k)}(x_0)}{k!}(x-x_0)^k" />  
When x is close to x<sub>0</sub> then <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;h(x)\approx&space;h(x_0)&plus;h'(x_0)(x-x_0)" title="h(x)\approx h(x_0)+h'(x_0)(x-x_0)" />

用泰勒级数解释Gradient Descent  
对于函数<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;y=f(\theta_1,\theta_2)" title="y=f(\theta_1,\theta_2)" />上的点(a,b),附近有点<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;(\theta_1,\theta_2)" title="(\theta_1,\theta_2)" />  
有  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;L(\theta)\approx&space;L(a,b)&plus;\frac{\partial&space;L(a,b)}{\partial\theta_1}(\theta_1-a)&plus;\frac{\partial&space;L(a,b)}{\partial\theta_2}(\theta_2-b)" title="L(\theta)\approx L(a,b)+\frac{\partial L(a,b)}{\partial\theta_1}(\theta_1-a)+\frac{\partial L(a,b)}{\partial\theta_2}(\theta_2-b)" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;s=L(a,b)" title="s=L(a,b)" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;u=\frac{\partial&space;L(a,b)}{\partial\theta_1}" title="u=\frac{\partial L(a,b)}{\partial\theta_1}" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;v=\frac{\partial&space;L(a,b)}{\partial\theta_2}" title="v=\frac{\partial L(a,b)}{\partial\theta_2}" />  
所以
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;L(\theta)\approx&space;s&plus;u(\theta_1-a)&plus;v(\theta_2-b)" title="L(\theta)\approx s+u(\theta_1-a)+v(\theta_2-b)" />

minimizing L(&theta;)  
(&theta;<sub>1</sub>,&theta;<sub>2</sub>)在圆内，则有  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;(\theta_1-a)^2&plus;(\theta_2-b)^2\leq&space;d^2" title="(\theta_1-a)^2+(\theta_2-b)^2\leq d^2" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;u(\theta_1-a)&plus;v(\theta_2-b)\\&space;\rightarrow&space;(\Delta\theta_1,\Delta\theta_2)\cdot(u,v)" title="u(\theta_1-a)+v(\theta_2-b)\\ \rightarrow (\Delta\theta_1,\Delta\theta_2)\cdot(u,v)" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\begin{bmatrix}&space;\Delta\theta_1\\&space;\Delta\theta_2&space;\end{bmatrix}&space;=-\eta&space;\begin{bmatrix}&space;u\\&space;v&space;\end{bmatrix}" title="\begin{bmatrix} \Delta\theta_1\\ \Delta\theta_2 \end{bmatrix} =-\eta \begin{bmatrix} u\\ v \end{bmatrix}" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\begin{bmatrix}&space;\theta_1\\&space;\theta_2&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;a\\&space;b&space;\end{bmatrix}&space;-\eta&space;\begin{bmatrix}&space;u\\&space;v&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;a\\&space;b&space;\end{bmatrix}&space;-\eta&space;\begin{bmatrix}&space;\frac{\partial&space;C(a,b)}{\partial\theta_1}\\&space;\frac{\partial&space;C(a,b)}{\partial\theta_2}&space;\end{bmatrix}" title="\begin{bmatrix} \theta_1\\ \theta_2 \end{bmatrix} = \begin{bmatrix} a\\ b \end{bmatrix} -\eta \begin{bmatrix} u\\ v \end{bmatrix} = \begin{bmatrix} a\\ b \end{bmatrix} -\eta \begin{bmatrix} \frac{\partial C(a,b)}{\partial\theta_1}\\ \frac{\partial C(a,b)}{\partial\theta_2} \end{bmatrix}" />

牛顿法  会考虑二次式

### More Limitation of Gradient Denscent
Gradient Descent的限制  
会卡在Local minima /saddle Point  
通常会在Loss接近0时就停止了

## 3-2 Gradient Descent Demo by AOE
[ML Lecture 3-2: Gradient Descent (Demo by AOE)](https://www.youtube.com/watch?v=1_HBTJyWgNA&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=7)

## 3-3 Gradient Descent Demo by Minecraft
[3-3 Gradient Descent (Demo by Minecraft)](https://www.youtube.com/watch?v=wzPAInDF_gI&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=8)