## 13 Unsupervised Learning - Linear Methods
[13 Unsupervised Learning - Linear Methods](https://www.youtube.com/watch?v=iwh5o_M4BNU&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=22)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/PCA%20(v3).pdf)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/MF.pdf)  

- Clustering & Dimension Reduction (集合和维度减少)  
- Generation

Dimension Reduction可以减少feature，得到关键特征输出  
Generate 可以给random，生成一个复杂的多维的‘原始数据’

### clustering
自动分类
#### K-means
- Clustering X={x<sup>1</sup>,...x<sup>n</sup>,x<sup>N</sup>} into K lusters
- Initialize cluster center c<sup>i</sup>,i=1,2,..K(K random x<sup>x</sup> from X)  
  设置类别1到K，随机取K个数据作为cluster的中心
- Repeat
  - For all x<sup>n</sup> in X: <a href="https://www.codecogs.com/eqnedit.php?latex=\bg_white&space;b_i^n=&space;\begin{cases}&space;1\qquad&space;x^n\text{&space;is&space;must&space;&quot;close&quot;&space;to&space;}c^i&space;\\&space;0\qquad&space;otherwise&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bg_white&space;b_i^n=&space;\begin{cases}&space;1\qquad&space;x^n\text{&space;is&space;must&space;&quot;close&quot;&space;to&space;}c^i&space;\\&space;0\qquad&space;otherwise&space;\end{cases}" title="b_i^n= \begin{cases} 1\qquad x^n\text{ is must &quot;close&quot; to }c^i \\ 0\qquad otherwise \end{cases}" /></a>
  - Updating all c<sup>i</sup>:<a href="https://www.codecogs.com/eqnedit.php?latex=\bg_white&space;c^i=\sum_{x^n}b_i^nx^n/\sum_{x^n}b_i^n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bg_white&space;c^i=\sum_{x^n}b_i^nx^n/\sum_{x^n}b_i^n" title="c^i=\sum_{x^n}b_i^nx^n/\sum_{x^n}b_i^n" /></a>

还是根据输入的分布自动做cluster??

#### Hierarchical Agglomerative Clustering
Hierarchical Agglomerative Clustering(HAC) 层次聚类
- build a tree
  - 计算最相似的data
  - 合并
- pick a threshold

对应该分几类提供了一个参考

- Distributed repersentation 发布表达式  
  高维的 多特征值的东西，用少维度的属性来表示 就是 Dimension Reduction

样本在不同分类特征中所占的权重
cluster A 0.75
cluster B 0.19
cluster C 0.06

### 如何 Dimension Reduction
1. feature selection (选择特征 直观的拿掉一个没什么数据的维度)
2. Principal Component Analysis (PCA 主要组成分析)  
   通过一堆多维的输入数据，找到转换矩阵，把数据降维  
   经过转换矩阵w<sup>1</sup>后，数据投影到维度轴上的奇异度最大  
   第二维要求与第一维度垂直 即 w<sup>1</sup> &middot; w<sup>2</sup> = 0  
   最后会找到一个orthogonal matrix 正交矩阵  
   可以使用 Lagrange multiplier (拉格朗日方程) 或者 Gradient Descent (梯度下降) 来求这降维矩阵  
   需要限制<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\left\|w^1&space;\right\|_2=1" title="\bg_white \left\|w^1 \right\|_2=1" />(即 2-norm / L2-regularization)

转换后的向量等于输入数据x乘以一个矩阵 W z = Wx

第一个主要特征维度的向量 z<sub>1</sub>  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;Var(z_1)&space;=&space;\sum_{z_1}(z_1&space;-&space;\overline{z_1})^2&space;\&space;\&space;\&space;\&space;\left\|w^1&space;\right\|_2=1" title="\bg_white Var(z_1) = \sum_{z_1}(z_1 - \overline{z_1})^2 \ \ \ \ \left\|w^1 \right\|_2=1" />


第二个主要特征维度的向量 z<sub>1</sub>  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\begin{align*}Var(z_1)&space;=&space;\sum_{z_1}(z_1&space;-&space;\overline{z_1})^2&space;\&space;\&space;\&space;\&space;&&&space;\left\|w^1&space;\right\|_2=1&space;\\&&&space;w^1&space;\cdot&space;w^2&space;=&space;0\end{align*}&space;" title="\bg_white \begin{align*}Var(z_1) = \sum_{z_1}(z_1 - \overline{z_1})^2 \ \ \ \ && \left\|w^1 \right\|_2=1 \\&& w^1 \cdot w^2 = 0\end{align*} " />

所有维度矩阵写一起  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;W&space;=&space;\begin{bmatrix}(w^1)^T&space;\\(w^2)^T&space;\\\vdots&space;\\\end{bmatrix}" title="\bg_white W = \begin{bmatrix}(w^1)^T \\(w^2)^T \\\vdots \\\end{bmatrix}" /> (orthogonal matrix 正交矩阵)


拉格朗日方程 解降维矩阵方法 或者 Gradient Descent 求解  ...

z = Wx  
经过降维转换后 z 的 covariance 会是一个 diagonal matrix ,使得数据decorrelation 不同 dimension 之间 covariance 为0  
如果结果z给其他模型使用时可以假设不同维度数据 没有 covariance <u>减少参数量</u>


<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\begin{align*}\textcircled{a}\\&(a\cdot&space;b)^2=(a^Tb)^2=a^Tba^Tb\\&&space;=a^Tb(a^Tb)=a^Tbb^Ta\end{align*}" title="\bg_white \begin{align*}\textcircled{a}\\&(a\cdot b)^2=(a^Tb)^2=a^Tba^Tb\\& =a^Tb(a^Tb)=a^Tbb^Ta\end{align*}" />

ⓑ 
**方差 协方差**
- 方差 var 描述期望和数据中心相同时的离散程度
- 协方差 cov 描述任意两个数据的离散程度

ⓒ S = Cov(x)

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;PAC:\\\begin{align*}&z_1=w^1\cdot&space;x&space;\\&\overline{z_1}=\sum&space;z_1=\sum&space;w^1&space;\cdot&space;x&space;=&space;w^1&space;\cdot&space;\sum&space;x&space;=&space;w^1&space;\cdot&space;\overline{x}\\\end{align*}&space;\\\begin{align*}&Var(z_1)&&=\sum_{z_1}(z_1-\overline{z_1})^2\\&&space;&&=\sum_x&space;(w^1\cdot&space;x-w^1\cdot\overline{x})^2\\&&space;&&=\sum(w^1)^T(x-\overline{x})(x-\overline{x})^Tw^1&space;&&space;\textcircled{a}\textcircled{b}\\&&space;&&=(w^1)^TCov(x)w^1&space;&&space;\textcircled{c}\\\end{align*}&space;\\" title="\bg_white PAC:\\\begin{align*}&z_1=w^1\cdot x \\&\overline{z_1}=\sum z_1=\sum w^1 \cdot x = w^1 \cdot \sum x = w^1 \cdot \overline{x}\\\end{align*} \\\begin{align*}&Var(z_1)&&=\sum_{z_1}(z_1-\overline{z_1})^2\\& &&=\sum_x (w^1\cdot x-w^1\cdot\overline{x})^2\\& &&=\sum(w^1)^T(x-\overline{x})(x-\overline{x})^Tw^1 & \textcircled{a}\textcircled{b}\\& &&=(w^1)^TCov(x)w^1 & \textcircled{c}\\\end{align*} \\" />  

#### find w1
最终得到 w<sup>1</sup> maximizing ： 
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;(w^1)^TSw^1\\&space;" title="\bg_white (w^1)^TSw^1\\ " />  
并且需要限制：<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\left\|w^1&space;\right\|_2=(w^1)^Tw^1=1" title="\bg_white \left\|w^1 \right\|_2=(w^1)^Tw^1=1" />

对于 S=Cov(x)有
1. Symmetric // 对称
2. Positive-semidefinite // 半正定
3. (non-negative eigenvalues ) // 非负特征值


使用 Lagrange multiplier 来解决  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;g(w^1)=(w^1)^TSw^1-\alpha&space;((x^1)^T-1)\\\left.\begin{matrix}\partial&space;g(W^1)/&space;\partial&space;w_1^1=0&space;\\\partial&space;g(W^1)/&space;\partial&space;w_2^1=0&space;\\\vdots\end{matrix}\right\}Sw^1-\alpha&space;w^1=0\\\begin{align*}Sw^1&=\alpha&space;w^1&space;\&space;\&space;&&&space;w^1:eigenvectior&space;\\&space;&=\alpha&space;\&space;\&space;&&&space;Choose\&space;the\&space;maximum\&space;one\end{align*}&space;" title="\bg_white g(w^1)=(w^1)^TSw^1-\alpha ((x^1)^T-1)\\\left.\begin{matrix}\partial g(W^1)/ \partial w_1^1=0 \\\partial g(W^1)/ \partial w_2^1=0 \\\vdots\end{matrix}\right\}Sw^1-\alpha w^1=0\\\begin{align*}Sw^1&=\alpha w^1 \ \ && w^1:eigenvectior \\ &=\alpha \ \ && Choose\ the\ maximum\ one\end{align*} " />


最终 W<sub>1</sub> 是 S 矩阵最大 eigenvalue (特征值) &lambda;<sub>1</sub> 对应的eigenvector (特征向量)

#### find w2

w<sup>2</sup> maximizing ： 
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;g(w^2)=(w^2)^TSw^2-\alpha&space;((w^2)^Tw^2-1)&space;-&space;\beta((w^2)^Tw^1-0)\\\left.\begin{matrix}\partial&space;g(W^2)/&space;\partial&space;w_1^2=0&space;\\\partial&space;g(W^2)/&space;\partial&space;w_2^2=0&space;\\\vdots\end{matrix}\right\}Sw^2-\alpha&space;w^2-\beta&space;w^1=0\\\begin{align*}(w^1)^TSw^2-\alpha&space;(w^1)^Tw^2-\beta&space;(w^1)^Tw^1&=0&space;\&space;\&space;\&space;&&space;ref:&space;*(w^1)^T\\(w^1)^TSw^2-\alpha&space;*0-\beta&space;*1&=0&space;\\\end{align*}\\\begin{align*}&&space;=((w^1)^TSw^2)^T=(w^2)^TS^Tw^1&space;\\&&space;=(w^2)^TSw^1&space;\&space;\&space;\&space;&&space;<=Sw^1&space;=&space;\lambda_1w^1\\&&space;=\lambda_1(w^2)^Tw^1&space;=&space;0&space;\end{align*}\\&space;\beta=0:&space;Sw^2-\alpha&space;w^2=0&space;\&space;\&space;\&space;\&space;\&space;\&space;Sw^2=\alpha&space;w^2&space;" title="\bg_white g(w^2)=(w^2)^TSw^2-\alpha ((w^2)^Tw^2-1) - \beta((w^2)^Tw^1-0)\\\left.\begin{matrix}\partial g(W^2)/ \partial w_1^2=0 \\\partial g(W^2)/ \partial w_2^2=0 \\\vdots\end{matrix}\right\}Sw^2-\alpha w^2-\beta w^1=0\\\begin{align*}(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta (w^1)^Tw^1&=0 \ \ \ & ref: *(w^1)^T\\(w^1)^TSw^2-\alpha *0-\beta *1&=0 \\\end{align*}\\\begin{align*}& =((w^1)^TSw^2)^T=(w^2)^TS^Tw^1 \\& =(w^2)^TSw^1 \ \ \ & <=Sw^1 = \lambda_1w^1\\& =\lambda_1(w^2)^Tw^1 = 0 \end{align*}\\ \beta=0: Sw^2-\alpha w^2=0 \ \ \ \ \ \ Sw^2=\alpha w^2 " />

w<sup>2</sup>是矩阵S 的 2<sup>nd</sup> eigenvalue (特征值) &lambda;<sub>2</sub> 对应的eigenvector (特征向量)

#### 计算结论
z=Wx  
Cov(z)=D // Diagonal matrix

PCA输出的data不同的dimension没有correnlation(相关性)可以让后面的模型用简单的模型来处理数据，减少 overfitting

// todo math

### PCA Another Point of View
PCA 另一种直观的描述

// \bar x x&#772;

// PCA 和一般的 deep learning 效果有什么区别？ 更多概率模型上的限制 对多维分散的数据友好 PCA是to,deep learning是to result的

每一个PCA分离出来的维度都是一个在数据中广泛存在的基础组件 Basic Component，

Basic Component is u  
x &ap; c<sub>1</sub>u<sup>1</sup> + c<sub>2</sub>u<sup>2</sup> + ... + c<sub>K</sub>u<sup>K</sup> + x&#772;

一张img 可以用<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\begin{bmatrix}c_1&space;\\c_2&space;\\\vdots&space;\\c_K\end{bmatrix}" title="\bg_white \begin{bmatrix}c_1 \\c_2 \\\vdots \\c_K\end{bmatrix}" />表示

x - x&#772; &ap; c<sub>1</sub>u<sup>1</sup> + c<sub>2</sub>u<sup>2</sup> + ... + c<sub>K</sub>u<sup>K</sup>  = x&#770;  

Reconstruction Error (重建错误):    
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\left\|&space;(x-\bar&space;x)-\hat&space;x&space;\right\|_2" title="\bg_white \left\| (x-\bar x)-\hat x \right\|_2" />

Error:  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;L=\min_{\{u^1,...,u^K\}}&space;\sum&space;\left\|&space;(x-\bar&space;x)-\sum_{k=1}^K&space;&space;c_ku^k&space;\right\|_2" title="\bg_white L=\min_{\{u^1,...,u^K\}} \sum \left\| (x-\bar x)-\sum_{k=1}^K c_ku^k \right\|_2" />

以上loss 的解就是PCA找到的解  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;PCA:&space;z=Wx\\\begin{bmatrix}z_1&space;\\z_2&space;\\\vdots&space;\\z_K\end{bmatrix}&space;=&space;\begin{bmatrix}(w_1)^T&space;\\(w_2)^T&space;\\\vdots&space;\\(w_K)^T\end{bmatrix}&space;x&space;" title="\bg_white PCA: z=Wx\\\begin{bmatrix}z_1 \\z_2 \\\vdots \\z_K\end{bmatrix} = \begin{bmatrix}(w_1)^T \\(w_2)^T \\\vdots \\(w_K)^T\end{bmatrix} x " />  
{w<sup>1</sup>,w<sup>2</sup>,...,w<sup>K</sup>} is the component  
{u<sup>1</sup>,u<sup>2</sup>,...,u<sup>K</sup>} minimizing L

![pca-matrix](./img/013-pca-matrix.jpg)  
黄色matrix 是原始数据 每一纵列即为每一笔data的 x<sup>i</sup>-x&#772;  
绿色matrix 是PCA分离出的特征矩阵，每一列即代表一个特征img(component)  
灰色matrix 是每一笔data对应pca特征的的权重，每一纵列即为每一笔data，一列里每一个数据代表这笔数据对应一个特征的权重。水平列数为几笔data的数量，行数为PCA特征component的数量

![pca-matrix-2](./img/013-pca-matrix-2.jpg)  

第一图中的component矩阵即2图的U矩阵， 一图的灰色矩阵即二图的 &Sigma;&sdot;V  
SVD: http://speech.ee.ntu.edu.tw/~tlkagk/courses/LA_2016/Lecture/SVD.pdf

要minimize reconstruction error <img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\hat&space;x&space;=&space;\sum_{k=1}^Kc_Kw^k&space;\&space;<==>&space;\&space;x-\bar&space;x" title="\bg_white \hat x = \sum_{k=1}^Kc_Kw^k \ <==> \ x-\bar x" />  
因为c<sub>k</sub>是orthonormal的  
所以 c<sub>k</sub>=(x-x&#772;)&sdot;w<sup>k</sup>

可以表示为一层的 nerual network, (x-x&#772;)乘以一个一层的nerual network,还是和之前的(x-x&#772;)比较接近，这个叫做 **Autoencoder**

![pca-matrix-2](./img/013-autoencoder-nerual.jpg)  

可以使用 Gradient Descent 代替PCA来得到 component matrix。但是没法保证 W<sup>k</sup> 之间是垂直的,recostructino error也不会比PCA更小。  
linear的情况下使用PCA会更快，但是unlinear的情况会使用 Gradient Descent 和多层的 hidden layer 来得到 component matrix (deep autoencoder)。


### PCA 的弱点

1. PCA 只考虑data的分布情况，data variance 最大，如果 分类之间的维度是有相关性的，分界线不是正交data variance分布的情况时，PCA降维后会使得分界消失(如果维度不够的话就不够还原原本的数据分布)。  
引入label 解决这个问题 LDA(linear discriminant analysis ) 是supervised的

2. PCA 是linear的，例如S型的高维数据会被投影混到一起，

### PCA 的实用

要使用多少个 principal component 取决于你的问题  
例如 做visualization,会project到2维  
计算每一个 principal component 的 eigenvalue &lambda; (特征值)，取影响大的维度。

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;ratio&space;=&space;\frac{\lambda_i}{\sum_{n=1}^k\lambda_n}" title="\bg_white ratio = \frac{\lambda_i}{\sum_{n=1}^k\lambda_n}" />

loulier 利群值

**PCA - MNIST**
在MNIST中 使用PCA得到的一个component weight 是一张图片 叫 Eigen-degit，做linear combination 就可以得到各种 degit

为什么PCA得到的weight是整张图片而不是由部分组成的component: weight 的值有正有负，更倾向于让方差最大 maximizing Covariance  
Non-negative Matrix Factorization (NMF) 非负值分解

### Matrix Factorization

不同的object有受同样的latent factor影响，

御宅数量 M，动漫人物数量 N，隐藏属性 K,
使用SVD求向量 r<sup>a-n</sup> r<sup>1-q</sup>  
![matrix-factorization](./img/013-matrix-factorization.jpg)

如果数据中含有未知的 missing data 数据，可以使用 Gradient descent 只计算有数据的data
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;L=\sum_{(i,j)}(r^i&space;\cdot&space;r^j-n_{ij})^2" title="\bg_white L=\sum_{(i,j)}(r^i \cdot r^j-n_{ij})^2" />  
r<sup>i</sup>御宅  r<sup>1-q</sup>动漫人物  
n<sub>ij</sub>购买的数量

更精确的描述：
r<sup>A</sup>&sdot;r<sup>1</sup>+b<sub>A</sub>+b<sub>1</sub>  
r<sup>A</sup> 御宅属性点  
r<sup>1</sup> 动漫人物属性点  
b<sub>A</sub> 御宅购买意愿  
b<sub>1</sub> 动漫人物畅销程度

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;L=\sum_{(i,j)}(r^i&space;\cdot&space;r^j&plus;b_i&plus;b_j-n_{ij})^2" title="\bg_white L=\sum_{(i,j)}(r^i \cdot r^j+b_i+b_j-n_{ij})^2" />

也可以加 regularization,如 r<sup>i</sup> r<sup>j</sup>是sparse的，只萌一个属性 加L1 regularization

用在 Topic analysis 上时叫做LSA(Latent Semantic Analysis)  
文档 - 词汇 - 词频 表 词频也可以乘上权重  
如何确定词频的权重，可以使用 inverse document frequency (IDF) 有多少比例的文档有使用这词汇

- Probability latent semantic analysis (PLSA)
- Latent Dirichlet Allocation (LDA)

- Multidimensional scaling (MDS) 不需要把每个data都表示为 feature vector,只要找到feature vector 和feature vector 的 distance 就可以做 Dimensionality reduction
- probabilistic PCA 机率问题PCA
- Kernel PCA 非线性版本PCA
- Canonical correlation Analysis (CCA) 不同类型的数据源 如声音和图像
- Independent Component Analysis (ICA) PCA找orthogonal 的component,ICA找independent 的component
- Linear Discriminant Analysis (LDA) supervise 的方法