## 13 Unsupervised Learning - Linear Methods
[13 Unsupervised Learning - Linear Methods](https://www.youtube.com/watch?v=iwh5o_M4BNU&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=22)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Structured%20Linear.pdf)

- Clustering & Dimension Reduction (集合和维度减少)  
- Generation

Dimension Reduction可以减少feature，得到关键特征输出  
Generate 可以给random，生成一个复杂的多维的‘原始数据’

### clustering
自动分类
#### K-means
- Clustering X={x<sup>1</sup>,...x<sup>n</sup>,x<sup>N</sup>} into K lusters
- Initialize cluster center c<sup>i</sup>,i=1,2,..K(K random x<sup>x</sup> from X)  
  设置类别1到K，随机取K个数据作为cluster的中心
- Repeat
  - For all x<sup>n</sup> in X: <a href="https://www.codecogs.com/eqnedit.php?latex=\bg_white&space;b_i^n=&space;\begin{cases}&space;1\qquad&space;x^n\text{&space;is&space;must&space;&quot;close&quot;&space;to&space;}c^i&space;\\&space;0\qquad&space;otherwise&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bg_white&space;b_i^n=&space;\begin{cases}&space;1\qquad&space;x^n\text{&space;is&space;must&space;&quot;close&quot;&space;to&space;}c^i&space;\\&space;0\qquad&space;otherwise&space;\end{cases}" title="b_i^n= \begin{cases} 1\qquad x^n\text{ is must &quot;close&quot; to }c^i \\ 0\qquad otherwise \end{cases}" /></a>
  - Updating all c<sup>i</sup>:<a href="https://www.codecogs.com/eqnedit.php?latex=\bg_white&space;c^i=\sum_{x^n}b_i^nx^n/\sum_{x^n}b_i^n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bg_white&space;c^i=\sum_{x^n}b_i^nx^n/\sum_{x^n}b_i^n" title="c^i=\sum_{x^n}b_i^nx^n/\sum_{x^n}b_i^n" /></a>

还是根据输入的分布自动做cluster??

#### Hierarchical Agglomerative Clustering
Hierarchical Agglomerative Clustering(HAC) 层次聚类
- build a tree
  - 计算最相似的data
  - 合并
- pick a threshold

对应该分几类提供了一个参考

- Distributed repersentation 发布表达式  
  高维的 多特征值的东西，用少维度的属性来表示 就是 Dimension Reduction

样本在不同分类特征中所占的权重
cluster A 0.75
cluster B 0.19
cluster C 0.06

### 如何 Dimension Reduction
1. feature selection (选择特征 直观的拿掉一个没什么数据的维度)
2. Principal Component Analysis (PCA 主要组成分析)  
   通过一堆多维的输入数据，找到转换矩阵，把数据降维  
   经过转换矩阵w<sup>1</sup>后，数据投影到维度轴上的奇异度最大  
   第二维要求与第一维度垂直 即 w<sup>1</sup> &middot; w<sup>2</sup> = 0  
   最后会找到一个orthogonal matrix 正交矩阵  
   可以使用 Lagrange multiplier (拉格朗日方程) 或者 Gradient Descent (梯度下降) 来求这降维矩阵  
   需要限制<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\left\|w^1&space;\right\|_2=1" title="\bg_white \left\|w^1 \right\|_2=1" />(即 2-norm / L2-regularization)

转换后的向量等于输入数据x乘以一个矩阵 W z = Wx

第一个主要特征维度的向量 z<sub>1</sub>  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;Var(z_1)&space;=&space;\sum_{z_1}(z_1&space;-&space;\overline{z_1})^2&space;\&space;\&space;\&space;\&space;\left\|w^1&space;\right\|_2=1" title="\bg_white Var(z_1) = \sum_{z_1}(z_1 - \overline{z_1})^2 \ \ \ \ \left\|w^1 \right\|_2=1" />


第二个主要特征维度的向量 z<sub>1</sub>  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\begin{align*}Var(z_1)&space;=&space;\sum_{z_1}(z_1&space;-&space;\overline{z_1})^2&space;\&space;\&space;\&space;\&space;&&&space;\left\|w^1&space;\right\|_2=1&space;\\&&&space;w^1&space;\cdot&space;w^2&space;=&space;0\end{align*}&space;" title="\bg_white \begin{align*}Var(z_1) = \sum_{z_1}(z_1 - \overline{z_1})^2 \ \ \ \ && \left\|w^1 \right\|_2=1 \\&& w^1 \cdot w^2 = 0\end{align*} " />

所有维度矩阵写一起  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;W&space;=&space;\begin{bmatrix}(w^1)^T&space;\\(w^2)^T&space;\\\vdots&space;\\\end{bmatrix}" title="\bg_white W = \begin{bmatrix}(w^1)^T \\(w^2)^T \\\vdots \\\end{bmatrix}" /> (orthogonal matrix 正交矩阵)


拉格朗日方程 解降维矩阵方法 或者 Gradient Descent 求解  ...

z = Wx  
经过降维转换后 z 的 covariance 会是一个 diagonal matrix ,使得数据decorrelation 不同 dimension 之间 covariance 为0  
如果结果z给其他模型使用时可以假设不同维度数据 没有 covariance <u>减少参数量</u>


<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\begin{align*}\textcircled{a}\\&(a\cdot&space;b)^2=(a^Tb)^2=a^Tba^Tb\\&&space;=a^Tb(a^Tb)=a^Tbb^Ta\end{align*}" title="\bg_white \begin{align*}\textcircled{a}\\&(a\cdot b)^2=(a^Tb)^2=a^Tba^Tb\\& =a^Tb(a^Tb)=a^Tbb^Ta\end{align*}" />

ⓑ 
**方差 协方差**
- 方差 var 描述期望和数据中心相同时的离散程度
- 协方差 cov 描述任意两个数据的离散程度

ⓒ S = Cov(x)

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;PAC:\\\begin{align*}&z_1=w^1\cdot&space;x&space;\\&\overline{z_1}=\sum&space;z_1=\sum&space;w^1&space;\cdot&space;x&space;=&space;w^1&space;\cdot&space;\sum&space;x&space;=&space;w^1&space;\cdot&space;\overline{x}\\\end{align*}&space;\\\begin{align*}&Var(z_1)&&=\sum_{z_1}(z_1-\overline{z_1})^2\\&&space;&&=\sum_x&space;(w^1\cdot&space;x-w^1\cdot\overline{x})^2\\&&space;&&=\sum(w^1)^T(x-\overline{x})(x-\overline{x})^Tw^1&space;&&space;\textcircled{a}\textcircled{b}\\&&space;&&=(w^1)^TCov(x)w^1&space;&&space;\textcircled{c}\\\end{align*}&space;\\" title="\bg_white PAC:\\\begin{align*}&z_1=w^1\cdot x \\&\overline{z_1}=\sum z_1=\sum w^1 \cdot x = w^1 \cdot \sum x = w^1 \cdot \overline{x}\\\end{align*} \\\begin{align*}&Var(z_1)&&=\sum_{z_1}(z_1-\overline{z_1})^2\\& &&=\sum_x (w^1\cdot x-w^1\cdot\overline{x})^2\\& &&=\sum(w^1)^T(x-\overline{x})(x-\overline{x})^Tw^1 & \textcircled{a}\textcircled{b}\\& &&=(w^1)^TCov(x)w^1 & \textcircled{c}\\\end{align*} \\" />  

#### find w1
最终得到 w<sup>1</sup> maximizing ： 
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;(w^1)^TSw^1\\&space;" title="\bg_white (w^1)^TSw^1\\ " />  
并且需要限制：<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;\left\|w^1&space;\right\|_2=(w^1)^Tw^1=1" title="\bg_white \left\|w^1 \right\|_2=(w^1)^Tw^1=1" />

对于 S=Cov(x)有
1. Symmetric // 对称
2. Positive-semidefinite // 半正定
3. (non-negative eigenvalues ) // 非负特征值


使用 Lagrange multiplier 来解决  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;g(w^1)=(w^1)^TSw^1-\alpha&space;((x^1)^T-1)\\\left.\begin{matrix}\partial&space;g(W^1)/&space;\partial&space;w_1^1=0&space;\\\partial&space;g(W^1)/&space;\partial&space;w_2^1=0&space;\\\vdots\end{matrix}\right\}Sw^1-\alpha&space;w^1=0\\\begin{align*}Sw^1&=\alpha&space;w^1&space;\&space;\&space;&&&space;w^1:eigenvectior&space;\\&space;&=\alpha&space;\&space;\&space;&&&space;Choose&space;the&space;maximum&space;one\end{align*}&space;" title="\bg_white g(w^1)=(w^1)^TSw^1-\alpha ((x^1)^T-1)\\\left.\begin{matrix}\partial g(W^1)/ \partial w_1^1=0 \\\partial g(W^1)/ \partial w_2^1=0 \\\vdots\end{matrix}\right\}Sw^1-\alpha w^1=0\\\begin{align*}Sw^1&=\alpha w^1 \ \ && w^1:eigenvectior \\ &=\alpha \ \ && Choose the maximum one\end{align*} " />


最终 W<sub>1</sub> 是 S 矩阵最大 eigenvalue (特征值) &lambda;<sub>1</sub> 对应的eigenvector (特征向量)

#### find w2

w<sup>2</sup> maximizing ： 
<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;g(w^2)=(w^2)^TSw^2-\alpha&space;((w^2)^Tw^2-1)&space;-&space;\beta((w^2)^Tw^1-0)\\\left.\begin{matrix}\partial&space;g(W^2)/&space;\partial&space;w_1^2=0&space;\\\partial&space;g(W^2)/&space;\partial&space;w_2^2=0&space;\\\vdots\end{matrix}\right\}Sw^2-\alpha&space;w^2-\beta&space;w^1=0\\\begin{align*}(w^1)^TSw^2-\alpha&space;(w^1)^Tw^2-\beta&space;(w^1)^Tw^1&=0&space;\&space;\&space;\&space;&&space;ref:&space;*(w^1)^T\\(w^1)^TSw^2-\alpha&space;*0-\beta&space;*1&=0&space;\\\end{align*}\\\begin{align*}&&space;=((w^1)^TSw^2)^T=(w^2)^TS^Tw^1&space;\\&&space;=(w^2)^TSw^1&space;\&space;\&space;\&space;&&space;<=Sw^1&space;=&space;\lambda_1w^1\\&&space;=\lambda_1(w^2)^Tw^1&space;=&space;0&space;\end{align*}\\&space;\beta=0:&space;Sw^2-\alpha&space;w^2=0&space;\&space;\&space;\&space;\&space;\&space;\&space;Sw^2=\alpha&space;w^2&space;" title="\bg_white g(w^2)=(w^2)^TSw^2-\alpha ((w^2)^Tw^2-1) - \beta((w^2)^Tw^1-0)\\\left.\begin{matrix}\partial g(W^2)/ \partial w_1^2=0 \\\partial g(W^2)/ \partial w_2^2=0 \\\vdots\end{matrix}\right\}Sw^2-\alpha w^2-\beta w^1=0\\\begin{align*}(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta (w^1)^Tw^1&=0 \ \ \ & ref: *(w^1)^T\\(w^1)^TSw^2-\alpha *0-\beta *1&=0 \\\end{align*}\\\begin{align*}& =((w^1)^TSw^2)^T=(w^2)^TS^Tw^1 \\& =(w^2)^TSw^1 \ \ \ & <=Sw^1 = \lambda_1w^1\\& =\lambda_1(w^2)^Tw^1 = 0 \end{align*}\\ \beta=0: Sw^2-\alpha w^2=0 \ \ \ \ \ \ Sw^2=\alpha w^2 " />

w<sup>2</sup>是矩阵S 的 2<sup>nd</sup> eigenvalue (特征值) &lambda;<sub>2</sub> 对应的eigenvector (特征向量)

#### 计算结论
z=Wx  
Cov(z)=D // Diagonal matrix

PCA输出的data不同的dimension没有correnlation(相关性)可以让后面的模型用简单的模型来处理数据，减少 overfitting

// todo math

