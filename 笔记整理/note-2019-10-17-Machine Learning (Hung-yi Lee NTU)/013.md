## 13 Unsupervised Learning - Linear Methods
[13 Unsupervised Learning - Linear Methods](https://www.youtube.com/watch?v=iwh5o_M4BNU&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=22)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Structured%20Linear.pdf)

- Clustering & Dimension Reduction (集合和维度减少)  
- Generation

Dimension Reduction可以减少feature，得到关键特征输出  
Generate 可以给random，生成一个复杂的多维的‘原始数据’

### clustering
自动分类
#### K-means
- Clustering X={x<sup>1</sup>,...x<sup>n</sup>,x<sup>N</sup>} into K lusters
- Initialize cluster center c<sup>i</sup>,i=1,2,..K(K random x<sup>x</sup> from X)  
  设置类别1到K，随机取K个数据作为cluster的中心
- Repeat
  - For all x<sup>n</sup> in X: <a href="https://www.codecogs.com/eqnedit.php?latex=\bg_white&space;b_i^n=&space;\begin{cases}&space;1\qquad&space;x^n\text{&space;is&space;must&space;"close"&space;to&space;}c^i&space;\\&space;0\qquad&space;otherwise&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bg_white&space;b_i^n=&space;\begin{cases}&space;1\qquad&space;x^n\text{&space;is&space;must&space;"close"&space;to&space;}c^i&space;\\&space;0\qquad&space;otherwise&space;\end{cases}" title="b_i^n= \begin{cases} 1\qquad x^n\text{ is must "close" to }c^i \\ 0\qquad otherwise \end{cases}" /></a>
  - Updating all c<sup>i</sup>:<a href="https://www.codecogs.com/eqnedit.php?latex=\bg_white&space;c^i=\sum_{x^n}b_i^nx^n/\sum_{x^n}b_i^n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bg_white&space;c^i=\sum_{x^n}b_i^nx^n/\sum_{x^n}b_i^n" title="c^i=\sum_{x^n}b_i^nx^n/\sum_{x^n}b_i^n" /></a>

还是根据输入的分布自动做cluster??

#### Hierarchical Agglomerative Clustering
Hierarchical Agglomerative Clustering(HAC)
- build a tree
  - 计算最相似的data
  - 合并
- pick a threshold

对应该分几类提供了一个参考

- Distributed repersentation 发布表达式  
  高维的 多特征值的东西，用少维度的属性来表示 就是 Dimension Reduction

样本在不同分类特征中所占的权重
cluster A 0.75
cluster B 0.19
cluster C 0.06

### 如何 Dimension Reduction
1. feature selection (选择特征 直观的拿掉一个没什么数据的维度)
2. Principal Component Analysis (PCA 主要组成分析)  
   通过一堆多维的输入数据，找到转换矩阵，把数据降维  
   经过转换矩阵w<sup>1</sup>后，数据投影到维度轴上的奇异度最大  
   第二维要求与第一维度垂直 即 w<sup>1</sup> &middot; w<sup>2</sup> = 0  
   最后会找到一个orthogonal matrix 正交矩阵  
   可以使用 Lagrange multiplier (拉格朗日方程) 或者 Gradient Descent (梯度下降) 来求这降维矩阵


拉格朗日方程 解降维矩阵方法 ...

z = Wx  
经过降维转换后 z 的 covariance 会是一个 diagonal matrix ,使得数据decorrelation 不同 dimension 之间 covariance 为0  
如果结果z给其他模型使用时可以假设不同维度数据 没有 covariance <u>减少参数量</u>


