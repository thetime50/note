## 023-1 Deep Reinforcement Learning 会出现的问题 里rning  
- Reward delay 反馈是延时的


[02-1 Deep Reinforcement Learning](https://www.youtube.com/watch?v=W8XF3ME8G2I&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=33)
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/RL%20(v6).pdf)

2015年2月 kreeger 在 Nature 上发了一篇 Reinforcement Learning 玩 Atari 的方法。  
16年 Alpha Go  

Agent: Observation(state) 观察到的 Environment,Action 影响环境  
Environment： 

胖dp, Part your observe,partial Observation State

```mermaid
graph TD
    Agent --Action--> Environment
    Environment --Reward--> Agent
    Environment --"Observation(state)"--> Agent

```

在下围棋的例子中，中间的Action一直没有 Reward,最终赢的时候才会有反馈，机器要自己知道过程中哪几步是好的，哪几步是不好的  

positive reward/ negative reward

用在 chat-bot 上，两个机器自动对话，让后人再评估  
再加上Gan  


OpenAI [Gym](https://gym.openai.com) / [Universe](https://openai.com/blog/universe/)


### Playing Video Game

Space invader

荧幕画面的 pixels 就是 observation  

从游戏的开始到结束 是一个 episode

observation s<sub>1</sub>  
action a<sub>1</sub>  
reward r<sub>1</sub>  

**Reinforcement Learning 里的问题**  
- Reward delay 反馈是延时的
    - 要做不是直接产生奖励但是会影响后面的奖励的事情
    - 做一些现在是不利但是对之后是有利的事情
- exploration 探索


### outline
- Markov Decision Process (MDP)
- deep Q Network
- **A3C**

- Policy based 方法
    - Learning an Actor
- Value based 方法
    - Learning a Critic
- Actor-Critic 方法
    - Asynchronous Advantage Actor-Critic (A3C)

Model based 

[video lecture](https://videolectures.net) 


### Actor

Actor = &pi;( Observation ) => Action  
  NN network  
  
做Policy Gradient 时 认为output 是 stochastic 的,输出的是执行动作的机率,就算是同样的场景也会采取不同的动作    

Action 的函数 &pi; 的参数为 &theta;, 输入为state s  
a = &pi;<sub>&theta;</sub>(s) 

- 机器看见 observation s<sub>1</sub>
- 机器决定 action a<sub>1</sub>
- 机器获得反馈 reward r<sub>1</sub>
- 机器看见 observation s<sub>2</sub>
- 机器决定 action a<sub>2</sub>
- 机器获得反馈 reward r<sub>2</sub>

最后 Maximize Total Reward  

#### 如何评估Actor的好坏

Total reward: <img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}R_\theta&space;=&space;\textstyle&space;\sum_{t=1}^T&space;r_t" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}R_\theta = \textstyle \sum_{t=1}^T r_t" />

r<sub>t</sub> 是每次step单次的回报

模型每次的输出的是机率，同样的参数和情景也有可能输出不同的值，游戏本身也是随机的，所以R<sub>&theta;</sub> 是随机值。而我们要 MAximize R<sub>T</sub> 的期望

- 一场比赛的轨迹为序列&tau;，里边包含 观察状态 行动 回报
    - <img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\tau&space;=&space;\{s_1,a_1,r_1,s_2,a_2,r_2,\cdots&space;,s_T,a_T,r_T\}" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\tau = \{s_1,a_1,r_1,s_2,a_2,r_2,\cdots ,s_T,a_T,r_T\}" />
    - 所有的回报 <img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}R(\tau)&space;=&space;\textstyle&space;\sum_{n=1}^N&space;r_n" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}R(\tau) = \textstyle \sum_{n=1}^N r_n" />
    - 一个游戏会有无数的过程，而model去玩游戏的时候只会看到很有限的一些过程，(还可能model很差，总是重复)

对于不同的Actor模型参数&theta;,某一种过程出现的机率：<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\bar{R_\theta}=\sum_\tau&space;R(\tau)P(\tau|\theta)" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\bar{R_\theta}=\sum_\tau R(\tau)P(\tau|\theta)" />  
对于某个的参数&theta;，Actor模型的总回报期望为，某种轨迹的总回报乘以这种轨迹出现的期望 再累加起来。

让Actor去(重复的)玩N场游戏得到 {&tau;<sup>1</sup>,&tau;<sup>2</sup>,...,&tau;<sup>N</sup>}，就好像从 P(&tau; | &theta;) 中采样了 N 笔轨迹序列  
最后某一个参数为 &theta; 的模型，能够获得的期望就近似为重复n场比赛获得回报的平均。

<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\bar{R_\theta}=\sum_\tau&space;R(\tau)P(\tau|\theta)\approx&space;\frac{1}{N}&space;\sum_{n=1}^N&space;R(\tau^n)" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\bar{R_\theta}=\sum_\tau R(\tau)P(\tau|\theta)\approx \frac{1}{N} \sum_{n=1}^N R(\tau^n)" />


用一个Actor 玩游戏生成序列，重复玩游戏作为training data, 得到平均回报用来评估，

(一笔data 里有几个轨迹序列)

#### 更新优化模型

Gradient Ascent:  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\theta^1&space;\leftarrow&space;\theta^0&space;&plus;&space;\eta&space;\nabla\bar{R_{\theta^0}}" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\theta^1 \leftarrow \theta^0 + \eta \nabla\bar{R_{\theta^0}}" />

<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\bar{R}_\theta&space;=&space;\sum_\tau&space;R(\tau)P(\tau|\theta)" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\bar{R}_\theta = \sum_\tau R(\tau)P(\tau|\theta)" />

R(&tau;) 对于 &theta; 是个常量  
现在考虑怎么计算 P(&tau;|&theta;)  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\nabla&space;\bar{R}_\theta&space;=&space;\sum_\tau&space;R(\tau)\nabla&space;P(\tau&space;|&space;\theta)&space;=&space;\sum_\tau&space;R(\tau)P(\tau&space;|&space;\theta)\frac{\nabla&space;P(\tau|\theta)}{P(\tau|\theta)}" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\nabla \bar{R}_\theta = \sum_\tau R(\tau)\nabla P(\tau | \theta) = \sum_\tau R(\tau)P(\tau | \theta)\frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}" />

<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}=\sum_\tau&space;R(\tau)P(\tau|\theta)\nabla&space;log&space;P(\tau|\theta)" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}=\sum_\tau R(\tau)P(\tau|\theta)\nabla log P(\tau|\theta)" />,   
<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\approx&space;\frac{1}{N}&space;\sum_{n=1}^N&space;R(\tau^n)\nabla&space;log&space;P(\tau^n&space;|&space;\theta)&space;" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\approx \frac{1}{N} \sum_{n=1}^N R(\tau^n)\nabla log P(\tau^n | \theta) " />,  
<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\\P(\tau&space;|&space;\theta)&space;=&space;\\&space;p(x_1)p(a_1|s_1,\theta)p(r_1,s_2&space;|&space;s_1,a_1)p(a_2|s_2,\theta)p(r_2,s_1&space;|&space;s_2,a_2)&space;\cdots&space;\\&space;=&space;p(s_1)\prod_{t=1}^{T}p(a_t|s_t,\theta)p(r_t,s_{t&plus;1}|s_t,a_t)" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\\P(\tau | \theta) = \\ p(x_1)p(a_1|s_1,\theta)p(r_1,s_2 | s_1,a_1)p(a_2|s_2,\theta)p(r_2,s_1 | s_2,a_2) \cdots \\ = p(s_1)\prod_{t=1}^{T}p(a_t|s_t,\theta)p(r_t,s_{t+1}|s_t,a_t)" />

其中 P(s<sub>1</sub>) 项 p(r<sub>t</sub>,s<sub>t+1</sub>|s<sub>t</sub>,a<sub>t</sub>) 项是和 Actor的 &theta; 是无关的。  

<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}log&space;P(\tau|\theta)&space;=&space;logp(s_1)&space;&plus;&space;\sum_{t=1}^T&space;logp(a_t|s_t,\theta)&plus;logp(r_t,s_{t&plus;1}|s_t,a_t)" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}log P(\tau|\theta) = logp(s_1) + \sum_{t=1}^T logp(a_t|s_t,\theta)+logp(r_t,s_{t+1}|s_t,a_t)" />

<img src="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\nabla&space;logP(\tau|\eta)&space;=&space;\sum_{t=1}^T&space;\nabla&space;logp(a_t,s_t,\theta)" title="https://latex.codecogs.com/gif.image?\dpi{110}\bg{white}\nabla logP(\tau|\eta) = \sum_{t=1}^T \nabla logp(a_t,s_t,\theta)" />

**T**为一场比赛step 的次数  
**N**为多少场游戏
![023-1-gradient-ascent](./img/023-1-gradient-ascent.jpg)

t 53"15'  

再某一场比赛轨迹 &tau;^n 中，在某次的观察状态 s_t^n 采取行动 a_t^n 后得到的机率是正的  
但是在式只中，某一点的机率是乘上整个轨迹&tau; 的 Reward R(&tau;<sup>n</sup>)   
为什么要取log? 在同样的轨迹下不能因为某种行为出现的概率更大而更优化这种情况，而忽略了出机率小但是得分可能更高的情况  


如果所有期望R(&tau;<sup>n</sup>)只有正的，没有惩罚怎么办？  
模型会让相同回报的行为出现的机率更平均。  

对于没出现过的情况 机率会减小 怎么处理？  
R(&tau;<sup>n</sup>) 减去一个bias (没有足够的收益就减小机率)


### Critic

从Critic 中得到一个Actor -> Q Learning

评估一个 observation 的状态有多好

Actor 和Critic 可以合在一起 train

MLDS  ??  

Actor Critic 可以得到什么， https://www.youtube.com/watch?v=nMR5mjCFZCw

VC-dimention

轩田
