## 14 Unsupervised Learning - Word Embedding

[14 Unsupervised Learning - Word Embedding](https://www.youtube.com/watch?v=X7PH3NuYW0Q&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=24)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/word2vec%20(v2).pdf)  


1-of-N encoding ( 像 1 hot encoding) 一个词只有一个含义，但是一个词是相关信息的组合  
建 Word Class (Dimension Reduction Clustering )  
Word Embedding 语义空间

### 如何做 Word Enbedding
看词汇在文章中的context

1. Count based 如果不同词汇在文章中同时出现，就相关  
    Glove Vector https://nlp.stanford.edu/projects/glove/
2. Pediction based learn 一个 neural network做单词机率预测Z,Z就可以作为Word Embedding  
    输入前 n 个词汇，输出下一个词汇  
    在一次预测输入的N个单词里 的1-of-N encoding的相同词汇轴对相同 neural 的weight也要相同  
    (有点像CNN的感觉，每个单词的词汇轴向量权重是共用的)  
    不会因为单词的位置影响语义网络的结构  
    z=W(w<sub>i-2</sub>+w<sub>i-1</sub>)  
    The lenght of x are both |V|,the length x are both |Z|, W are both |Z| x |V| matries  
    对同样的weight使用同样的初始值，做gradient descent 时可以减去把所有w位置的gradient累加起来的和


