## 14 Unsupervised Learning - Word Embedding

[14 Unsupervised Learning - Word Embedding](https://www.youtube.com/watch?v=X7PH3NuYW0Q&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=24)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/word2vec%20(v2).pdf)  


1-of-N encoding ( 像 1 hot encoding) 一个词只有一个含义，但是一个词是相关信息的组合  
建 Word Class (Dimension Reduction Clustering )  
Word Embedding 语义空间

### 如何做 Word Embedding
看词汇在文章中的context

1. Count based 如果不同词汇在文章中同时出现，就相关  
    Glove Vector https://nlp.stanford.edu/projects/glove/
2. Pediction based learn 一个 neural network做单词机率预测Z,Z就可以作为Word Embedding  
    输入前 n 个词汇，输出下一个词汇  
    在一次预测输入的N个单词里 的1-of-N encoding的相同词汇轴对相同 neural 的weight也要相同  
    (有点像CNN的感觉，每个单词的词汇轴向量权重是共用的)  
    不会因为单词的位置影响语义网络的结构  
    z=W(w<sub>i-2</sub>+w<sub>i-1</sub>)  
    The lenght of x are both |V|,the length x are both |Z|, W are both |Z| x |V| matries  
    对同样的weight使用同样的初始值，做gradient descent 时可以减去把所有w位置的gradient累加起来的和


- Continuous Bag of Words (CBOW) 使用两侧的词汇预测中间的词汇
- Skip-gram 使用中间的词汇预测两侧的词汇


为什么propose word vector 不是deep的，因为不是deep 运算量少可以跑很多数据

相同类型的两个词汇 word vector 相减结果会相距很近  
所有动词的3态在图像上会有相同的形态

可以拿两个不同语言词汇的 word vector和已知的词汇对应关系来learn一个model来做word vector 转换 翻译  
可以对影像做 embedding,让图片和 word vector 对应上 (用已知的图片来做 image - word vector 转换model,有了转换model 即使是没分类过的图片也可以通过转换model来得到结果)

document embedding 把一个word 变成一个vector  
1. bag of words  
    document -> bag of words -> word vector ( semantic embedding ) 但是没法解决词汇顺序影响文本含义的问题
2. Paragraph Vector
3. Seq2seq Auto-encoder
4. Skip Thought
5. 其他  


1. 已有的learn好模型应用做产品
2. 已有的ML模型,自己用数据learn模型 (lib? toolkit?)
3. 改造已有模型
4. 已有的文献写模型
5. 从头搭建


