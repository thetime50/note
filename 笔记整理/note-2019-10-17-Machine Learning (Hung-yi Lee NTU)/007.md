## 7 Backpropagation
[ML Lecture 7: Backpropagation](https://www.youtube.com/watch?v=ibJpTrp5mcE&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=12)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/BP.pdf)

Backpropagation是计算大量Gradient的方法

### Chain Rule
规则链

case 1
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;y=g(x)&space;z=h(y)\\&space;\Delta&space;x\rightarrow&space;\Delta&space;y&space;\rightarrow&space;\Delta&space;z&space;\qquad&space;\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}" title="\\ y=g(x) z=h(y)\\ \Delta x\rightarrow \Delta y \rightarrow \Delta z \qquad \frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}" />

case 2
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;x=g(s)&space;y=h(s)&space;z=k(x,y)\\&space;\begin{align*}&space;&\qquad\&space;\Delta&space;x\\&space;&\quad&space;\nearrow\quad\&space;\searrow\\&space;&\Delta&space;s&space;\qquad\qquad\Delta&space;z\\&space;&\quad&space;\searrow\quad\&space;\nearrow\\&space;&\qquad\&space;\Delta&space;y&space;\end{align}&space;\qquad&space;\frac{dz}{ds}=\frac{\partial&space;z}{\partial&space;x}\frac{dx}{ds}&plus;\frac{\partial&space;z}{\partial&space;y}\frac{dy}{ds}" title="\\ x=g(s) y=h(s) z=k(x,y)\\ \begin{align*} &\qquad\ \Delta x\\ &\quad \nearrow\quad\ \searrow\\ &\Delta s \qquad\qquad\Delta z\\ &\quad \searrow\quad\ \nearrow\\ &\qquad\ \Delta y \end{align} \qquad \frac{dz}{ds}=\frac{\partial z}{\partial x}\frac{dx}{ds}+\frac{\partial z}{\partial y}\frac{dy}{ds}" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;x^n\rightarrow&space;NN\&space;\theta\rightarrow&space;y^n\underset{C^n}{\Leftrightarrow}&space;\hat&space;y^n" title="x^n\rightarrow NN\ \theta\rightarrow y^n\underset{C^n}{\Leftrightarrow} \hat y^n" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\begin{align*}&space;L(\theta)=\sum_{n=1}^N&space;C^n(\theta)&space;\end{align}&space;\quad\Rightarrow&space;\quad&space;\begin{align*}&space;\frac{\partial&space;L(\theta)}{\partial&space;w}=\sum_{n=1}^{N}\frac{\partial&space;C^n(\theta)}{\partial&space;w}&space;\end{align}" title="\begin{align*} L(\theta)=\sum_{n=1}^N C^n(\theta) \end{align} \quad\Rightarrow \quad \begin{align*} \frac{\partial L(\theta)}{\partial w}=\sum_{n=1}^{N}\frac{\partial C^n(\theta)}{\partial w} \end{align}" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;z=x_1&space;w_1&plus;x_2&plus;w_2&plus;b\\&space;\\&space;\frac{\partial&space;C}{\partial&space;w}=\frac{\partial&space;z}{\partial&space;w}\frac{\partial&space;C}{\partial&space;z}" title="z=x_1 w_1+x_2+w_2+b\\ \\ \frac{\partial C}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial C}{\partial z}" />

Forward pass:&part;z/&part;w  
Backward pass:&part;C/&part;z 

&part;z/&part;w<sub>i</sub> =x<sub>i</sub>  

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;a=\sigma(z)\\\\&space;\frac{\partial&space;C}{\partial&space;z}=\frac{\partial&space;z}{\partial&space;z}\frac{\partial&space;C}{\partial&space;a}\\\\&space;(1)\&space;\frac{\partial&space;z}{\partial&space;z}=\sigma'(z)\\\\&space;(2)\&space;\frac{\partial&space;C}{\partial&space;a}=&space;\frac{\partial&space;z'}{\partial&space;a}\frac{\partial&space;C}{\partial&space;z'}&plus;&space;\frac{\partial&space;z''}{\partial&space;a}\frac{\partial&space;C}{\partial&space;z''}\quad\text{(...&space;next&space;layer&space;neuron)}" title="\\ a=\sigma(z)\\\\ \frac{\partial C}{\partial z}=\frac{\partial z}{\partial z}\frac{\partial C}{\partial a}\\\\ (1)\ \frac{\partial z}{\partial z}=\sigma'(z)\\\\ (2)\ \frac{\partial C}{\partial a}= \frac{\partial z'}{\partial a}\frac{\partial C}{\partial z'}+ \frac{\partial z''}{\partial a}\frac{\partial C}{\partial z''}\quad\text{(... next layer neuron)}" />