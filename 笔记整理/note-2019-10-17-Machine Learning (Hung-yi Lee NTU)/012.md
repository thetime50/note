## 12 Semi-supervised
[12 Semi-supervised](https://www.youtube.com/watch?v=fX_guE7JNnY&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=21)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/semi%20(v3).pdf)

- Supervised learning<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\{(x^rm\het&space;y^r)\}_{r=1}^R" title="\{(x^rm\het y^r)\}_{r=1}^R" />
- Semi-supervised learning<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\{(x^rm\het&space;y^r)\}_{r=1}^R,\{x^u\}_{u=R}^{R&plus;U}" title="\{(x^rm\het y^r)\}_{r=1}^R,\{x^u\}_{u=R}^{R+U}" /> U>>R (U unlabel data)  
  - Transduction learning:Unlabel data is the testing data  
    testing data 的 feature 训练是可以的，不能使用 testing data 的 label 训练(相当是更于倾向于针对 testing data 进行label的训练? maybe not )
  - Inductive learning:Unlabel data is not testing data


### Outline
- Semi-supervised Learning for Generative Model
- Low-density Separation Assumption
- Smoothness Assumption
- Better Representation( 放到 Supervised learning 里边再说)

### Semi-Supervised Generative Model
**Supervised Generative Model**
- Given labelled training examples x<sup>r</sup>&isin;C<sub>1</sub>,C</sub>2</sub>
  - 找到最大先验概率P(C</sub>i</sub>)和class-dependent probability P(x|C</sub>i</sub>)
  - P(x|C</sub>i</sub>)is a Gaussian Parameterized by &mu;</sup>i</sup> and &Sigma;

With P(C</sub>1</sub>),P(C</sub>2</sub>),&mu;</sup>1</sup>,&mu;</sup>2</sup>,&Sigma;  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)&plus;P(x|C_2)P(C_2)}" title="P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}" />

**Semi-Supervised Generative Model**
- Initialization:<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\theta=\{P(C_1),P(C_2),\mu^1,\mu^2,\Sigma\}" title="\theta=\{P(C_1),P(C_2),\mu^1,\mu^2,\Sigma\}" />(random or 估测)
- Step 1:compute the posterior probability of unlabeled data  
  <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;P_\theta(C_1|x^\mu)" title="P_\theta(C_1|x^\mu)" /> Depending on model &theta;
- Step 2:updata model  
  <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;P_(C_1)=\frac{N_1&plus;\Sigma_{x^u}P(C_1|x^u)}{N}" title="P_(C_1)=\frac{N_1+\Sigma_{x^u}P(C_1|x^u)}{N}" />  
  N:total number of examples  
  N<sub>1</sub>:number of examples belonging to C<sub>1</sub>  
  <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\mu^1=\frac{1}{N_1}\sum_{x^r\in&space;C_1}x^r&plus;\frac{1}{\sum_{x^u}P(C_1|x^u)}\sum_{x^u}P(C_1|x^u)x^u" title="\mu^1=\frac{1}{N_1}\sum_{x^r\in C_1}x^r+\frac{1}{\sum_{x^u}P(C_1|x^u)}\sum_{x^u}P(C_1|x^u)x^u" />  
  pre EM algorithm  
  ***back to step 1*** 

<pre>
about P(C<sub>1</sub>)
P(C<sub>1</sub>)等于 label data 中分类为1的数量加上 unlabel data 中分类为1的数量 除以总数。
unlabel data 中数分类为1的数量为 unlabel data 输入带入发布模型得到的概率
P(C<sub>1</sub>) 模型已经用随机的参数初始化了，后面再不断迭代更新

&mu;<sup>1</sup> 是分类为1的input data概率发布的平均值
对于label data 中为 class 1 的input data 平均值加 unlabel data 中class 1 的 input data 平均值
unlabel data 中class 1 的 input data 平均值为输入数据乘以分类为 class 1 的期望，除以unlabel data 中class 1数量
</pre>

- Maximum likelihood with labelled data  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;logL(\theta)=\sum_{x^r}logP_\theta(x^r,\hat&space;y^r)\\&space;P_\theta(x^r,\hat&space;y^r)=P_\theta(x^r|\hat&space;y^r)P(\hat&space;y^r)" title="\\ logL(\theta)=\sum_{x^r}logP_\theta(x^r,\hat y^r)\\ P_\theta(x^r,\hat y^r)=P_\theta(x^r|\hat y^r)P(\hat y^r)" />
- Maximum likelihood weith labelled+unlabel data  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;logL(\theta)=\sum_{x^r}logP_\theta(x^r,\hat&space;y^r)&plus;\sum_{x^u}logP_\theta(x^u)\\&space;P_\theta(x^u)=P_\theta(x^u|C_1)P(C_1)&plus;P\theta(x^u|C_2)P(C_2)" title="\\ logL(\theta)=\sum_{x^r}logP_\theta(x^r,\hat y^r)+\sum_{x^u}logP_\theta(x^u)\\ P_\theta(x^u)=P_\theta(x^u|C_1)P(C_1)+P\theta(x^u|C_2)P(C_2)" />

### Low-Density Separation
以低密度的地方作为边界

**Self-training**  
1. 从 label data 中 train 一个model出来 即f<sup>*</sup>(使用 neural network/deep/shallow等方法都可以)
2. 用f<aup>*</sup>计算出 unlabel data 的 Pseudo-label
3. 自己设计一些规则 拿出 unlabel data 的数据放到 label data set 中可以给添加的数据再加上权重  
  再回去train f</sup>*</suo> model

Self trainling data 这种做法对Regression 问题没什么效果，Regression问题model计算出来的模型值和模型本身是重合的，<del>classification问题通过softmax转换后有一定的调整空间(输出值不止1个，输出是阈值)</del>

self-training 的时候会强制分配一笔data 一定会属于其中一个class的  (hard label)  
在Generative model 的时候是不同的class都有各自的posterior probability,分别计算属 于各分类的概率(soft label)

对于计算结果\[0.7,0.3\]  
Hard: train class 1，输出target固定\[1,0\] or \[0,1\]然后拿去做 cross entropy  
soft: 70%-class1 30%-class2 输出target\[0.7,0.3\](这值拿回去做 self training 是没有效果的，总的来说 self-training待会精确值是没有效果的)

**Entropy-based Regularizetion**

testing data Entropy:<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;E(y^u)=-\sum_{m=1}^5y_m^uln(y_m^u)" title="E(y^u)=-\sum_{m=1}^5y_m^uln(y_m^u)" />  
(out y =0 or y=1 ln(y)=0)

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;L=\sum_xC(y^r,\hat&space;y^r)\quad&space;labled\&space;data\\&space;.\&space;&plus;\lambda\sum_{x^u}E(y^u)\qquad&space;unlabel\&space;data" title="\\ L=\sum_xC(y^r,\hat y^r)\quad labled\ data\\ .\ +\lambda\sum_{x^u}E(y^u)\qquad unlabel\ data" />

倾向于对 lebel data 的 output 于 label 进行匹配，对于 unlabel data 计算出来的distribute倾向于集中到某一个class(倾向于数据集中 边界density)

因为有U>>R 所以加入 test data 的 entropy 避免 model 对 label data 的 overfitting

**Semi-supervised SVM**  
outlook

穷举label的可能性
1. 边界距离两个class越大越好
2. 最小分类错误

又改用一个替代穷举的方法：每次修改一比 unlabeled data 看 objective function 变大，可以变大的才确定做出修改

### Smoothness Assumption
1. x的分布是不平均的
2. 如果x<sub>1</sub> x<sub>2</sub>在一个高密度的区域内且x<sub>1</sub> x<sub>2</sub>距离很近，那么他们的 y hat 也是接近的(观察x<sub>1</sub>到x<sub>2</sub>之间的是否有 high density 的 path) 

(通过输入的分布来推算输出的分布，mabey有一些限制，受sample影响)
不同class可能很像，同一class可能不像

一般需要*deep autoencoder* call feature,然后再call clustering

***Cluster and then label***  
做Cluster，匹配label

***Graph-based Approach***  
用图表来处理高密度链接问题

为什么要算singularity  
计算edge

- 有些数据自带graph 的连接，比如网页分类的hyperlink 论文分类的引用关系
- 自己建立graph关系
  - 定义object之间怎么算相似度
  - add edge:
    - K Nearest Neighbor (连接最近的k个点)
    - e-Neighborhood (相似度超过一定阈值的才会相连)
  - 连接点之间的edge可以带weight
    - RBM function<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;s(x^i,x^j)=exp(-\gemma||x^i-x^j||^2)" title="s(x^i,x^j)=exp(-\gemma||x^i-x^j||^2)" />(Euclidean distace )(exp 的连接对较远的点衰减更快)

data要够多要连续，否则可能会连接不到(基于model的输出分布情况)

定量描述：  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;S=1/2\sum_{i,j}w_{i,j}(y^i-y^j)^2=\mathbf{y}^TL\mathbf{y}" title="S=1/2\sum_{i,j}w_{i,j}(y^i-y^j)^2=\mathbf{y}^TL\mathbf{y}" />  
对所有的数据(label data 和 unlabel data)任意取两点i和j，w是两点之间的权重(和两point的input data距离有关)，(y<sup>i</sup>-y<sup>j</sup>)<sup>2</sup>是输出值之差的平方

<pre>
input point 和point之间的connect(edge)是由上面add edge规则来决定的(距离太远的点就不会建立连接)，
w<sub>i,j</sub>是由input data和edge决定的，edge固定后就不会变了
learn出来的model要让使按距离关系连接在一起的input data输出的label也更接近
</pre>

- **y**:(R+U)-dim vector  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\mathbf&space;y=[...y^i...y^j...]^\mathbf&space;T" title="\mathbf y=[...y^i...y^j...]^\mathbf T" />
- L:(R+U)*(R+U) matrix (Graph Laplacian)  
L=D-W  
W=[w<sup>i,j</sup>...] (每个point 两两之间的权重关系组成的 matrix)  
L 是W每一行的和放在对角线组成matrix，一个point和其他所有point 的weight之和

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;S=\textbf&space;y^T&space;L&space;\textbf&space;y\\&space;L=\sum_{x^r}C(y^r,\hat&space;y^r)&plus;\lambda&space;S" title="\\ S=\textbf y^T L \textbf y\\ L=\sum_{x^r}C(y^r,\hat y^r)+\lambda S" />

&lambda;S like Regularization term

*前边&sum;项即普通的交叉熵等损失函数，后边的&lambda;S项是edge weight和对应y的差值的平方(f(&Delta;x)&sdot;&Delta;y<sup>2</sup>)*

moothness可以加在hidden layer的任何一层或者是多层做embedding layer

**Better Representation**  
find the latent factors
(更好的 更简单的表示)


### 小结

- Semi-supervised Learning for Generative Model
- Low-density Separation Assumption
  - Self-training  
    把 unlabel data带入模型定义过滤规则 加上权重再带回去train
  - Entropy-based Regularizetion  
    倾向模型能够于把point概率集中到某一个class上
  - SVM
- Smoothness Assumption
  - Cluster and then label
  - Graph-based Approach
    - add edge:
      - K Nearest Neighbor (连接最近的k个点)
      - e-Neighborhood (相似度超过一定阈值的才会相连)
    - 连接点之间的edge可以带weight
      - RBM
    - edge + weight定义loss functon
- Better Representation( 放到 Supervised learning 里边再说)