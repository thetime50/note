# note-2019-10-17-Machine Learning (Hung-yi Lee NTU)

youtobe: https://www.youtube.com/watch?v=CXgbekl66jc&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49  
課程網頁: http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html  
https://pan.baidu.com/s/1Shjn2el7gr3RPNlUTnBtxw  
https://speech.ee.ntu.edu.tw/~tlkagk/courses.html  
bili: https://www.bilibili.com/video/av65521101/?p=1  

## 0-1 Introduction of Machine Learning
[ML Lecture 0-1: Introduction of Machine Learning](https://youtu.be/CXgbekl66jc)  
[课件](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017_2/Lecture/introduction.pdf)  
[参考](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017_2/Lecture/policy.pdf)  

人工智能AI -> 机器学习ML -> 深度学习DL

Training：

Model  
衡量结果 -> 演化演算 -> 训练后的模型 测试使用
训练数据  

![LearningMap](img/LearningMap.jpg)

** learning scenario **

- Supervised Learning 有监督学习  
  需要大量train data - input/output pair of target function
  .
  - Regression回归 一种ML的task 输出是scalar/real number的function
  - Classification分类器 
    - Binary Classification 二元分类器
    - multi-classification
    - /
    - Linear Model
    - Non-linear Model
      - Deep Learning (Hierarchical Structure)
      - SVM,decision tree,K-NN
  - Structured Learning (输出结构化的数据 语音识别 翻译 人脸)
    - GAN 对抗网络
- Semi-supervised Learning 半监督学习  
  labelled data and unlabeled data
- Tranfer Learning 迁移学习
  - other labelled data,other unlabeled data,
- Unsupervised Learning 无监督学习
- Reinforcemont Learning 增强学习 (game AlphaGo)  
  只有最终结果的情况下对之间过程学习分析
  learning from critics

- scenario 情景 方案
  - task
    - method (model)

## 0-2 Why we need to learn machine learning?
[ML Lecture 0-2: Why we need to learn machine learning?](https://youtu.be/On1N8u1z2Ng)

## 1 回归 - 案例研究
[ML讲座1：回归 - 案例研究](https://youtu.be/fegAeph9UaA)
[课件](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/Regression.pdf)

<!-- https://www.codecogs.com/latex/eqneditor.php -->
<!-- escape('').replace(/\+/g, '&plus;'); -->

#### step1 Model

set of function:  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;y=b&plus;w\cdot&space;x_{cp}" title="y=b+w\cdot x_{cp}" />

Linear model:  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;y=b&plus;\sum{w_i&space;x_i}" title="y=b+\sum{w_i x_i}" />

- x<sub>i</sub>: an sttribute of input x *feature*
- w<sub>i</sub>: weight
- b<sub>i</sub>: bias

#### step2 Goodness of Function

样本：  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;(x^{i}_{cp},\widehat{y}^&space;{i})" title="(x^{i}_{cp},\widehat{y}^ {i})" />

- i:第i组数据
- ![\widehat{-}](https://latex.codecogs.com/gif.latex?\bg_white&space;\widehat{-}): hat表示来自样本的输出(scalar)

```
Model(linear)
衡量结果(loss function) -> 演化演算 -> 训练后的模型 测试使用
训练数据

```

Loss function L:  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\begin{array}{lcl}&space;L(f)&space;&&space;=&space;&&space;L(w,b)\\&space;&&space;=&space;&&space;\sum_{n=1}^{10}(\widehat{y}^{n}-(b&plus;w\cdot&space;x^{n}_{cp}))^{2}&space;\end{array}" title="\begin{array}{lcl} L(f) & = & L(w,b)\\ & = & \sum_{n=1}^{10}(\widehat{y}^{n}-(b+w\cdot x^{n}_{cp}))^{2} \end{array}" />

#### setp3 Best Function

pick the "Best" Function :  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;f^{*}=arg&space;\&space;\min_{f}&space;L(f)" title="f^{*}=arg \ \min_{f} L(f)" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\begin{array}{lcl}&space;w^{*},b^{*}&space;&&space;=&space;&&space;arg&space;\&space;\min_{w,b}&space;L(f)&space;\\&space;&&space;=&space;&&space;arg&space;\min_{w,b}\sum_{n=1}^{n}(\widehat{y}^{n}-(b&plus;w&space;\cdot&space;x_{cp}^{n}))^{n}&space;\end{array}" title="\begin{array}{lcl} w^{*},b^{*} & = & arg \ \min_{w,b} L(f) \\ & = & arg \min_{w,b}\sum_{n=1}^{n}(\widehat{y}^{n}-(b+w \cdot x_{cp}^{n}))^{n} \end{array}" />

- closed-form solution 封闭解
- ***gradient descent*** 梯度下降

consider loss functino L(w) with one parameter w:<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;w^*=arg&space;\min_w&space;{L(w)}" title="w^*=arg \min_w {L(w)}" />

1. (Randomly) Pick an initial value w<sup>0</sup>
2. Compute <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\frac{dL}{dw}|_{w=w_0}" title="\frac{dL}{dw}|_{w=w_0}" />  
  if Negative then Increase w  
  if Positive then Decrease w  
  w<sup>1</sup> &lt;= <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;w^0-\eta\frac{dL}{dw}|_{w=w^0}" title="w^0-\eta\frac{dL}{dw}|_{w=w^0}" /> \eta is called "leanrin rate"
- Many iteration to Local optimal (linear regression不用考虑Local optimal问题)

auto two parameters:<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;w*,b*=arg&space;\min_{w,b}{L(w,b)}" title="w*,b*=arg \min_{w,b}{L(w,b)}" />  
1. (Randomly) Pick an initial value w<sup>0</sup>,b<sup>0</sup>
2. Compute <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\frac{\partial&space;L}{\partial&space;w}|_{w=w_0,b=b_0}" title="\frac{\partial L}{\partial w}|_{w=w_0,b=b_0}" />,<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\frac{\partial&space;L}{\partial&space;d}|_{w=w_0,b=b_0}" title="\frac{\partial L}{\partial d}|_{w=w_0,b=b_0}" />  
  w<sup>1</sup> &lt;== <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;w^0-&space;\eta\frac{\partial&space;L}{\partial&space;w}|_{w=w^0,b=b^0}" title="w^0- \eta\frac{\partial L}{\partial w}|_{w=w^0,b=b^0}" />  
  b<sup>1</sup> &lt;== <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;b^0-&space;\eta\frac{\partial&space;L}{\partial&space;w}|_{w=w^0,b=b^0}" title="b^0- \eta\frac{\partial L}{\partial w}|_{w=w^0,b=b^0}" />
3. Many iteration to Local optimal

***梯度向量***(等高线的法线向量) <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\nabla&space;L\begin{bmatrix}&space;\frac{\partial&space;L}{\partial&space;w}\\&space;\frac{\partial&space;L}{\partial&space;d}&space;\end{bmatrix}_{grdient}" title="\nabla L\begin{bmatrix} \frac{\partial L}{\partial w}\\ \frac{\partial L}{\partial d} \end{bmatrix}_{grdient}" />

- formulation of <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\partial&space;L&space;/&space;\partial&space;w" title="\partial L / \partial w" /> and <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\partial&space;L&space;/&space;\partial&space;b" title="\partial L / \partial b" />

<img src="https://latex.codecogs.com/gif.image?\dpi{110}&space;\bg_white&space;L(w,b)=\sum_{n=1}^{a}(\hat{y}^m-(b&plus;w\cdot&space;x_{cp}^n))^2" title="\bg_white L(w,b)=\sum_{n=1}^{a}(\hat{y}^m-(b+w\cdot x_{cp}^n))^2" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\frac{\partial&space;L}{\partial&space;w}=\sum&space;_{n=1}^{10}2(\hat{y}^n-(b&plus;w\cdot&space;x_{cp}^n))(-x_{cp}^n)" title="\frac{\partial L}{\partial w}=\sum _{n=1}^{10}2(\hat{y}^n-(b+w\cdot x_{cp}^n))(-x_{cp}^n)" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\frac{\partial&space;L}{\partial&space;b}=\sum&space;_{n=1}^{10}2(\hat{y}^n-(b&plus;w\cdot&space;x_{cp}^n))(-1)" title="\frac{\partial L}{\partial b}=\sum _{n=1}^{10}2(\hat{y}^n-(b+w\cdot x_{cp}^n))(-1)" />

How's the results?  
Generalization  
对于一般的数据结果如何  - 引入testing data

模型过于复杂时 - Overfitting问题

***分类x<sub>s</sub> 多变量问题***  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;y=b&plus;\sum&space;w_i&space;x_i" title="y=b+\sum w_i x_i" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\begin{array}{lcl}&space;y=&space;&&&space;b_1*\delta&space;(x_s=Pidgey)&plus;w_1*\delta(x_s=Pidgey)x_{cp}&plus;\\&space;&&&space;b_2*\delta(x_s=Weedl)&plus;w_2*\delta(x_s=Weedl)x_{cp}&space;\end{array}" title="\begin{array}{lcl} y= && b_1*\delta (x_s=Pidgey)+w_1*\delta(x_s=Pidgey)x_{cp}+\\ && b_2*\delta(x_s=Weedl)+w_2*\delta(x_s=Weedl)x_{cp} \end{array}" />

#### Regularization
正则化
loss function中考虑error and smooth

model:  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;y=b&plus;\sum&space;w_i&space;x_i" title="y=b+\sum w_i x_i" />  
loss:  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;L=\sum_n(\hat&space;y^n-(b&plus;\sum&space;w_i&space;x_i))^2&plus;\lambda\sum(w_i)^2" title="L=\sum_n(\hat y^n-(b+\sum w_i x_i))^2+\lambda\sum(w_i)^2" />

将w<sub>i</sub><sup>2</sup> 加入loss function,让参数趋向小，图像平滑避免过拟合  
Regularization 不用考虑 b

#### 小结
- step1 Model  
  y=kx+b 或者高次项
- step2 Goodness of Function  
  Loss function 
- setp3 Best Function  
  Gradient descent
- Overfitting 模型内参数过多过拟合  
  Regularization 给参数加Loss权重 正则化 平滑

## 1 Regression - Demo
[ML Lecture 1: Regression - Demo](https://www.youtube.com/watch?v=1UqCjFQiiy0&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=4)

讲解  
Code for Demo: http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/demo/GradientDescentDemo.ipynb

demo中使用了Adagrad  
(解决不同参数应该使用不同的更新速率的问题)  
分别对w和d偏微分的平方累加 Ir_b=Ir_b+b_grad  
技术计算下一轮w和d时使用b=b-Ir/np.sqrt(Ir_b)*b_grad