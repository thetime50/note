## 8-1 Hello world of deep learning
[8-1 Hello world of deep learning](https://www.youtube.com/watch?v=Lx3l4lOrquw&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=13)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017_2/Lecture/keras.pdf)

TensorFlow and theano 会比较 flexible ,算微分用的  
所以，今天用的是Keras

希腊语的牛角 梦精灵项目 梦精灵从象牙的门出来梦就不会实现 如果从牛角的梦出来就会实现

MNIST Data:http://yann.lecun.com/exdb/mnist/  
Keras provides data sets loading function: http://keras.io/datasets/

1. model function set
```python
model = Sequential()

model.add(Dense(input_dim=28*28,output_dim=500)) #Fully Connect layer
model.add(Activation("sigmoid")) # softplus, softsign, sigmoid,tanh, hard_sigmoid, linear …

model.add(Dense(output_dim=500))
model.add(Activation("sigmiod"))

model.add(Dens(output_10))
model.add(Activation("softmax"))
```
2. goodness function (evaluate)
```python
model.compile(loss='categorical crossentropy',
	optimizer="adam",
	metrics=['accuracy']
)
```
Several alternatives: https://keras.io/objectives/  
3. pick the best function
```python
model.fit(x_train,y_train,batch_size=100,nb_epoch=20)
```
x_train,y_train:is numpy array  
batch_size:GPU并行批处理，每一批更新一次数据  
nb_epoch:学习几轮

- 随机划分batch,按随机的顺序计算更新  
- if Batch size=1 then is Stochastic gradient descent  
- batch size大相对稳定  
- <u>batch size大容易进local minima</u>,Stochastic有一定随机性可以跳过一些local minima

Save and load models：https://faroit.github.io/keras-docs/2.0.2/getting-started/faq/#howcan-i-save-a-keras-model

```python
score=model.evaluate(x_test,y_test)
print('Total loss on Testing Set:',score[0])
print('Accuracy of Testing Set:',score[1])
```

```python
result=model.predict(x_test)
```

## 8-2 Keras 2-0
[8-2 Keras 2-0](https://www.youtube.com/watch?v=5BJDJd-dzzg&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=14)

```python
model=Sequential()

model.add(Dense(input_dim=28*28,units=500, #this diff with keras 1.x
	activation='reul'
))
model.add(Dense(units=500,
	activation='relu'))
model.add(Dense(units=10,
	activation='softmax'))

###

model.compile(loss='categorital crossentropy',
	optimiser='adam'
	metrics=["accuracy"])

model.fit(x_train,y_train,batch_size,epochs=20)

score=model.evaluate(x_test,y_test)
print("Total loss on Testing Set:",score[0])
print("Accuracy of Testing Set:",score[1])

result=model.predict(x_test)
```

## 8-3 Keras Demo
[8-3: Keras Demo](https://www.youtube.com/watch?v=L8unuZNpWw8&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=15)

```python
(x_train,y_train),(x_test,y_test)=load_data()

model.add(Dense(input_dim=28*28,uints=689,activation="sigmoid"))
model.add(Dense(units=689.activation="sigmoid"))
#for i in range(10):
model.add(Dense(units=10,activation="softmax"))

model.compile(
	loss="mse",#Mean Square Error
	optimizer=SGD(lr=0.1) #Gradient descent
	metrics=["accuracy"])

model.fit(x_train,y_train,batch_size=100,epochs=20)

result=model.evaluate(x_test,y_test)
print("Total loss on Testing Set:",result[0])
print("Accuracy of Testing Set:",result[1])

```