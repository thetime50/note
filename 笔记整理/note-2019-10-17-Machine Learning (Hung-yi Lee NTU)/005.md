## 5 Logistic Regression
[ML Lecture 5: Logistic Regression](https://www.youtube.com/watch?v=hSXFuypLukA&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=10)  
[pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf)

### function set
需要找到一个后验概率(posterior probulibily)<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;P_{w,b}(C_1|x)" title="P_{w,b}(C_1|x)" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\begin{align*}&space;if\&space;P_{w,b}(C_1|x)\geq&space;0.5&,&space;output\&space;C_1&space;\\&space;Otherwise&,output\&space;C_2&space;\end{align*}" title="\begin{align*} if\ P_{w,b}(C_1|x)\geq 0.5&, output\ C_1 \\ Otherwise&,output\ C_2 \end{align*}" />

使用Gaussian模型表示:  
(使用其他模型化简后可以得到同样结果)

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;P_{w,b}(C_1|x)=\sigma(z)\\&space;\\&space;\sigma(z)=\frac{1}{1-exp(-z)}\\&space;\\&space;z=w\cdot&space;x&plus;b=\sum_i&space;w_ix_i&plus;b\\" title="\\ P_{w,b}(C_1|x)=\sigma(z)\\ \\ \sigma(z)=\frac{1}{1-exp(-z)}\\ \\ z=w\cdot x+b=\sum_i w_ix_i+b\\" />

![function-setjpg](./img/005-function-setjpg.jpg)  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;f_{w,b}=P_{w,b}(C_1|x)" title="f_{w,b}=P_{w,b}(C_1|x)" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;L(a,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))...f_{w,b}(x^N)" title="L(a,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))...f_{w,b}(x^N)" />

### loss function

The most likely w<sup>\*</sup> and  b<sup>*</sup> is the one with the largest L(w,b)  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;w^*,b^*=arg&space;\max_{w,b}L(w,b)" title="w^*,b^*=arg \max_{w,b}L(w,b)" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\Rightarrow&space;w^*,b^*=arg&space;\min_{w,b}-lnL(w,b)" title="\Rightarrow w^*,b^*=arg \min_{w,b}-lnL(w,b)" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\Rightarrow&space;w^*,b^*&space;=arg&space;\min_{w,b}-lnL(w,b)" title="\Rightarrow w^*,b^* =arg \min_{w,b}-lnL(w,b)" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;-lnL(w,b)=-lnL_{w,b}(x^1)-lnL_{w,b}(x^2)-ln(1-L_{w,b}(x^1))..." title="-lnL(w,b)=-lnL_{w,b}(x^1)-lnL_{w,b}(x^2)-ln(1-L_{w,b}(x^1))..." />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;output\&space;C_1&space;:\&space;\hat&space;y=1\\&space;then&space;:\\&space;-lnL(w,b)=\sum_i\underbrace{-[\hat&space;y^ilnf(x^i)&plus;(1-\hat&space;y^i)ln(1-f(x^i))]}_{\mbox{Cross&space;entropy&space;between&space;two&space;Bernoulli&space;distribution}}" title="\\ output\ C_1 :\ \hat y=1\\ then :\\ -lnL(w,b)=\sum_i\underbrace{-[\hat y^ilnf(x^i)+(1-\hat y^i)ln(1-f(x^i))]}_{\mbox{Cross entropy between two Bernoulli distribution}}" />  

*Cross entropy*
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;\begin{array}{|c|c|}&space;mbox{Distribute&space;p:}&&space;mbox{Distribute&space;q:}\\&space;p(x=1)=\hat&space;y^n&space;&&space;a(x=1)=f(x^n)\\&space;p(x=0)=1-\hat&space;y^n&space;&&space;a(x=0)=1-f(x^n)&space;\end{array}\\&space;\mbox{cross&space;entropu&space;is}\\&space;H(p,q)=-\sum_xp(x)ln(q(x))" title="\\ \begin{array}{|c|c|} mbox{Distribute p:}& mbox{Distribute q:}\\ p(x=1)=\hat y^n & a(x=1)=f(x^n)\\ p(x=0)=1-\hat y^n & a(x=0)=1-f(x^n) \end{array}\\ \mbox{cross entropu is}\\ H(p,q)=-\sum_xp(x)ln(q(x))" />  
两个distribution接近程度

Cross entribute:  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;C(f(x^n),\hat&space;y^n)=-[\hat&space;y^ilnf(x^i)&plus;(1-\hat&space;y^i)ln(1-f(x^i))]" title="C(f(x^n),\hat y^n)=-[\hat y^ilnf(x^i)+(1-\hat y^i)ln(1-f(x^i))]" />

Loss function:  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;L(f)=\sum_nC(f(x^n),\hat&space;y^n)" title="L(f)=\sum_nC(f(x^n),\hat y^n)" />

为什么不用square error

### fnid the best function
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\frac{\partial(-lnL(w,b))}{\partial&space;w_i}=\sum_n-\Bigl[\hat&space;y^n\frac{\partial&space;lnf_{w,b}(x^n)}{\partial&space;w_i}&plus;(1-\hat&space;y^n)\frac{\partial&space;ln(1-f_{w,b}(x^n))}{\partial&space;w_i}\Bigl]" title="\frac{\partial(-lnL(w,b))}{\partial w_i}=\sum_n-\Bigl[\hat y^n\frac{\partial lnf_{w,b}(x^n)}{\partial w_i}+(1-\hat y^n)\frac{\partial ln(1-f_{w,b}(x^n))}{\partial w_i}\Bigl]" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;\frac{\partial&space;lnf_{w,b}(x)}{\partial&space;w_i}=\frac{\partial&space;lnf_{w,b}(x)}{\partial&space;z}\frac{\partial&space;z}{\partial&space;w_i}\\&space;\\&space;\frac{\partial&space;z}{\partial&space;w_i}=x_i\\&space;\\&space;\begin{align*}&space;\frac{\partial&space;lnf_{w,b}(x^n)}{\partial&space;z}&=\frac{\partial&space;ln\sigma(z)}{\partial&space;z}\\&space;&=\frac{1}{\sigma(z)}\frac{\partial\sigma(z)}{\partial&space;z}\\&space;&=\frac{1}{\sigma(z)}\sigma(z)(1-\sigma(z))=(1-\sigma(z))&space;\end{align*}" title="\\ \frac{\partial lnf_{w,b}(x)}{\partial w_i}=\frac{\partial lnf_{w,b}(x)}{\partial z}\frac{\partial z}{\partial w_i}\\ \\ \frac{\partial z}{\partial w_i}=x_i\\ \\ \begin{align*} \frac{\partial lnf_{w,b}(x^n)}{\partial z}&=\frac{\partial ln\sigma(z)}{\partial z}\\ &=\frac{1}{\sigma(z)}\frac{\partial\sigma(z)}{\partial z}\\ &=\frac{1}{\sigma(z)}\sigma(z)(1-\sigma(z))=(1-\sigma(z)) \end{align*}" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\frac{\partial&space;lnf_{w,b}(x)}{\partial&space;w_i}=(1-\sigma(z))x_i=(1-f_{w,b}(x^n))x_i^n" title="\frac{\partial lnf_{w,b}(x)}{\partial w_i}=(1-\sigma(z))x_i=(1-f_{w,b}(x^n))x_i^n" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\frac{\partial&space;ln(1-f_{w,b}(x^n))}{\partial&space;w_i}=\sigma(z)x_i=f_{w,b}(x^n)x_i^n" title="\frac{\partial ln(1-f_{w,b}(x^n))}{\partial w_i}=\sigma(z)x_i=f_{w,b}(x^n)x_i^n" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\frac{-lnL(w,b)}{\partial&space;w_i}=\sum_n-(\hat&space;y^n-f_{w,b}(x^n))x_i^n" title="\frac{-lnL(w,b)}{\partial w_i}=\sum_n-(\hat y^n-f_{w,b}(x^n))x_i^n" />

<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;w_i&space;\leftarrow&space;w_i-\eta\sum_n-(\hat&space;y^n-f_{w,b}(x^n))x_i^n" title="w_i \leftarrow w_i-\eta\sum_n-(\hat y^n-f_{w,b}(x^n))x_i^n" />

### Logistic Regression and Square Error
如果使用方差作为loss function 那么有

model:  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;f_{w,b}(x)=\sigma(\sum_i&space;w_i&space;x_i&plus;b)" title="f_{w,b}(x)=\sigma(\sum_i w_i x_i+b)" />  
Training data: <img src="https://latex.codecogs.com/gif.latex?\bg_white&space;(x^n,\hat&space;y^n),\hat&space;y^n" title="(x^n,\hat y^n),\hat y^n" />:1 for class1,0 for class2  
loss function:  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;L(f)=1/2\sum_n(f_{w,b}(x^n)-\hat&space;y^n)^2" title="L(f)=1/2\sum_n(f_{w,b}(x^n)-\hat y^n)^2" />  
带入样本左偏微分：  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\begin{align*}&space;\frac{\partial(f_{w,b}(x)-\hat&space;y)^2}{\partial&space;w_i}&space;&=&space;2(f_{w,b}(x)-\hat&space;y)\frac{\partial&space;f_{w,b}(x)}{\partial&space;z}\frac{\partial&space;z}{\partial&space;w_i}\\&space;&=2(f_{w,b}(x)-\hat&space;y)f_{w,b}(x)(1-f_{w,b}(x))x_i&space;\end{align*}" title="\begin{align*} \frac{\partial(f_{w,b}(x)-\hat y)^2}{\partial w_i} &= 2(f_{w,b}(x)-\hat y)\frac{\partial f_{w,b}(x)}{\partial z}\frac{\partial z}{\partial w_i}\\ &=2(f_{w,b}(x)-\hat y)f_{w,b}(x)(1-f_{w,b}(x))x_i \end{align*}" />  
带入样本计算偏微分总是<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\partial&space;L/\partial&space;w_i=0" title="\partial L/\partial w_i=0" />(因为里面的f(x)&cdot;(1-f(x)))  
square error在图形上离目标较远的得分存在高地，无法通过偏微分找到目标

### Discriminative vs Generative
Logistic Regrission -> Discriminative 的方法
Gaussian Distribution -> Generative 的方法

function set model:  
P(c<sub>1</sub>|x)=&sigma;(w&cdot;x+b)

find the best function
Logistic Regrission :gradient descent找w,b
Gaussian Distribution :计算&mu;<sup>1</sup>,&mu;<sup>2</sup>,&sum;<sup>-1</sup>，然后计算w,b (假设了Gaussian/Bernoulli/naive)

数据少的情况下Generative效果常常比Discriminative效果要好
(参数量要少，可能性要少，更倾向给出更有规律的可能，过滤掉多的特征)

![005-g-vs-d](./img/005-g-vs-d.jpg)

使用niave bayes计算：  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\\&space;P(C_1)=\frac{1}{13}&space;\quad&space;P(x_1=1|C_1)=1&space;\quad&space;P(x_2=1|C_1)=1\\&space;P(C_2)\frac{12}{13}&space;\quad&space;P(x_1=1|C_2)=\frac{1}{3}&space;\quad&space;P(x_2=1|C_2)=\frac{1}{3}" title="\\ P(C_1)=\frac{1}{13} \quad P(x_1=1|C_1)=1 \quad P(x_2=1|C_1)=1\\ P(C_2)\frac{12}{13} \quad P(x_1=1|C_2)=\frac{1}{3} \quad P(x_2=1|C_2)=\frac{1}{3}" />  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)&plus;P(x|C_2)P(C_2)}" title="P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}" />
最终P(C<sub>1</sub>|x)&lt;0.5

生成性模型的好处
- 在假设概率分布的情况下，减少了对训练数据的需求，增强了对噪声的鲁棒性
- 通过概率分布的假设，可以从不同的来源估计出更强的噪声先验和类相关概率。(抗噪声 noise)
- 先验和类相关的概率可以从不同的来源估计。

- discriminative  
 假设posterior probulibily找参数

- Generative  
  formulation拆成Priors 和 class-dependent probulibily  
  可以通过不同来源计算得到参数(语言识别使用neuralnetwork 属于discriminative的方法 但是语言识别属于Generative的系统，可以通过收集文字信息计算Priors)


### Multi-class Classfication
vector 
<!-- scheler?? -->
inner product

Softmax会将数据限制在0-1之间加强大小数据的差异

Maximum Entropy类似 Logistic Regression的分类方法

![005-multi-classification.jpg](./img/005-multi-classification.jpg)

### Limitation of Logistic Regression

![005-cascading-model.jpg](./img/005-cascading-model.jpg)

neural network