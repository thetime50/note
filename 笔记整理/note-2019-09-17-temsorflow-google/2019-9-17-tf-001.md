# 2019-9-17-tf-001 教程
## 开始使用 TensorFlow
- [TensorFlow Core](https://tensorflow.google.cn/tutorials/)
- [Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb)
- [Colab: An easy way to learn and use TensorFlow](https://medium.com/tensorflow/colab-an-easy-way-to-learn-and-use-tensorflow-d74d1686e309)
- [tensorflow youtube](https://www.youtube.com/tensorflow)

- [keras](https://keras.io/zh/)

## 学习和使用机器学习
### 概览
- [Book: Deep Learning with Python](https://books.google.com/books?id=Yo3CAQAACAAJ)
- [机器学习速成课程](https://developers.google.cn/machine-learning/crash-course/)

### 基本分类
[link->](https://tensorflow.google.cn/tutorials/keras/basic_classification)

[开始时使用 TensorFlow](#开始使用-tensorflow)demo

```python
import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0 # 调整数据格式 这里是做了运算符重载?

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)), # 展开图片(二维数组)
  tf.keras.layers.Dense(512, activation=tf.nn.relu), # 512个节点 稠密链接层或全连接层
  tf.keras.layers.Dropout(0.2), # 下降梯度??
  tf.keras.layers.Dense(10, activation=tf.nn.softmax) # 输出节点 softmax回归
])
model.compile(optimizer='adam', # 优化器 
              loss='sparse_categorical_crossentropy', # 损失函数
              metrics=['accuracy']) # 显示的评估结果数据

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
```


```python
from __future__ import absolute_import, division, print_function, unicode_literals

# 导入TensorFlow和tf.keras
import tensorflow as tf
from tensorflow import keras

# 导入辅助库
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# 显示图片
plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()

#处理图片数据
train_images = train_images / 255.0

test_images = test_images / 255.0
# 显示图片和名称
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()

# 配置模型层
model=keras.Sequential([
  keras.layes.Flatten(input_shape=(28,28)),
  keras.layes.Dense(128,activation=tf.nn.relu),
  keras.layes.Dense(10,activation=tf.nn.softmax)
])

model.compile(optimizer='adam',
              loss='spares_categorical_crossentropy',
              metrics=['accuraty'])

model.fit(train_images,train_labels.epochs=5)
test_loss, test_acc=model.evaluste(test_images,test.labels)
print('test accuracy',test_acc,"loss",test_loss)#测试数据精度低于训练精度说明过拟合

# 进行预测
predictions = model.predict(test_iamges[:20])
print(predictions[0],np.argmax(prediction[0]),test_lables[0])

# 图表显示
def plot_image(i,predictions_array,true_label,img):
  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'
  plt.xlabel("{} {:2.0f} ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                              color=color)

def plot_value_array(i, predictions_array, true_label):
  predictions_array, true_label = predictions_array[i], true_label[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0,1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

i = 0
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions, test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions, test_labels)
plt.show()

num_rows=5
num_cols=3
num_images = num_rows*num_cols
plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_image(i, predictions, test_labels, test_images)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_value_array(i, predictions, test_labels)
plt.show()

#预测单个数据
img = test_images[0]
img = (np.expand_dims(img,0)) # 包装一下
print(test_images[0],img)
predictions_single = model.predict(img)
print(predictions_single)
plot_value_array(0, predictions_single, test_labels)
plt.xticks(range(10), class_names, rotation=45)
plt.show()
prediction_result = np.argmax(predictions_single[0])
print(prediction_result)
```

### 影评文本分类-tfHub
[link->](https://tensorflow.google.cn/tutorials/keras/text_classification_with_hub)

```python
#!python36
# https://tensorflow.google.cn/tutorials/keras/text_classification_with_hub
from __future__ import absolute_import, division, print_function, unicode_literals

import numpy as np

import tensorflow as tf

import tensorflow_hub as hub
import tensorflow_datasets as tfds

print("Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.experimental.list_physical_devices("GPU") else "NOT AVAILABLE")

# 将训练集按照 6:4 的比例进行切割，从而最终我们将得到 15,000
# 个训练样本, 10,000 个验证样本以及 25,000 个测试样本
train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])

(train_data, validation_data), test_data = tfds.load(
    name="imdb_reviews", 
    split=(train_validation_split, tfds.Split.TEST),
    as_supervised=True)

train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))
train_examples_batch
train_labels_batch

#使用预训练的text embedding文本嵌入模型将文本转换为embedding vectos嵌入向量
embedding = "https://hub.tensorflow.google.cn/google/tf2-preview/gnews-swivel-20dim/1"
# embedding = "google/tf2-preview/gnews-swivel-20dim-with-oov/1" # 2.5%的词汇转换为未登录词桶（OOV buckets）
# embedding = "google/tf2-preview/nnlm-en-dim50/1" # 1M 50维
# embedding = "google/tf2-preview/nnlm-en-dim128/1" # 1M 128维
hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=True)
hub_layer(train_examples_batch[:3])

# 模型层
model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.summary()

# 训练
history = model.fit(train_data.shuffle(10000).batch(512),
                    epochs=20,
                    validation_data=validation_data.batch(512),
                    verbose=1)

# 评估
results = model.evaluate(test_data.batch(512), verbose=2)
for name, value in zip(model.metrics_names, results):
  print("%s: %.3f" % (name, value))
```
使用嵌入向量
- 不必自己文本预处理
- 从迁移学习中受益 （正交的维度空间，互相关联）
- 嵌入具有固定长度，更易于处理

### 影评文本分类-预处理文本
[link->](https://tensorflow.google.cn/tutorials/keras/text_classification)

```python
#!python36
# https://tensorflow.google.cn/tutorials/keras/text_classification
import tensorflow as tf
from tensorflow import keras

import numpy as np

print(tf.__version__)

# 加载数据
imdb = keras.datasets.imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) # 映射前10000的词
# _data 高频词映射索引
# _labels 0 或 1 负面/正面

word_index = imdb.get_word_index()#单词-索引映射表
word_index = {k:(v+3) for k,v in word_index.items()} #开头带单引号的数据是什么 原本后面的数值是什么
word_index["<PAD>"] = 0
word_index["<START>"] = 1
word_index["<UNK>"] = 2  # unknown
word_index["<UNUSED>"] = 3

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):
  return ' '.join([reverse_word_index.get(i, '?') for i in text])

#show etxt
# print(decode_review(train_data[0]))

# 影评数组转换为张量
# 标准化数据
train_data = keras.preprocessing.sequence.pad_sequences(train_data,
  value=word_index["<PAD>"],
  padding='post',
  maxlen=256)

test_data = keras.preprocessing.sequence.pad_sequences(test_data,
  value=word_index["<PAD>"],
  padding='post',
  maxlen=256)

  # input shape is the vocabulary count used for the movie reviews (10,000 words)
vocab_size = 10000

model = keras.Sequential()
model.add(keras.layers.Embedding(vocab_size, 16)) # 将单词索引转换到向量空间
model.add(keras.layers.GlobalAveragePooling1D()) # AGP 全局平均池化 减少数据量获取特征图从而减少过拟合
model.add(keras.layers.Dense(16, activation=tf.nn.relu)) # 隐藏层 全链接层
model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))

model.summary()

# 损失函数和优化器
model.compile(optimizer=tf.train.AdamOptimizer(),
              loss='binary_crossentropy', #二分类交叉熵 # mean_squared_error #均方差
              metrics=['accuracy'])

# 验证集
x_val = train_data[:10000]
partial_x_train = train_data[10000:]

y_val = train_labels[:10000]
partial_y_train = train_labels[10000:]

# 训练
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=40,
                    batch_size=512,# 批大小?
                    validation_data=(x_val, y_val),# 测试数据
                    verbose=1) # 冗余?
# 评估
results = model.evaluate(test_data, test_labels)
print(results)

# 训练过程信息
history_dict = history.history
history_dict.keys()

import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)
# loss图表 过拟合后测试集的损失函数结果计算结果升高
# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
# 准确性图表
plt.clf()   # clear figure
acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()
```

### 回归问题
Basic regression: Predict fuel efficiency
```python
#!python36
# https://tensorflow.google.cn/tutorials/keras/regression
from __future__ import absolute_import, division, print_function, unicode_literals

import pathlib

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

print(tf.__version__) # 2.0.0

dataset_path = keras.utils.get_file("auto-mpg.data", "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")
# dataset_path

column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',
                'Acceleration', 'Model Year', 'Origin']
raw_dataset = pd.read_csv(dataset_path, names=column_names,
                      na_values = "?", comment='\t',
                      sep=" ", skipinitialspace=True)

dataset = raw_dataset.copy()
# dataset.tail()
# dataset.isna().sum()
dataset = dataset.dropna() # 删除空数据行
origin = dataset.pop('Origin') # 获取Origin列数据
dataset['USA'] = (origin == 1)*1.0 # 转为one hot编码
dataset['Europe'] = (origin == 2)*1.0
dataset['Japan'] = (origin == 3)*1.0
# dataset.tail()

# 拆分训练数据和测试数据
train_dataset = dataset.sample(frac=0.8,random_state=0)
test_dataset = dataset.drop(train_dataset.index)
# 查看数据
sns.pairplot(train_dataset[["MPG", "Cylinders", "Displacement", "Weight"]], diag_kind="kde")
plt.show()

train_stats = train_dataset.describe()
train_stats.pop("MPG")
train_stats = train_stats.transpose()
train_stats
# 分离特征值
train_labels = train_dataset.pop('MPG')
test_labels = test_dataset.pop('MPG')
# 数据规范化
def norm(x):
  return (x - train_stats['mean']) / train_stats['std']
normed_train_data = norm(train_dataset)
normed_test_data = norm(test_dataset)
# 构建模型
def build_model():
  model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
  ])

  optimizer = tf.keras.optimizers.RMSprop(0.001)

  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae', 'mse'])
  return model

model = build_model()
# model.summary() # 显示模型定义
# 预测
example_batch = normed_train_data[:10]
example_result = model.predict(example_batch)
example_result

# 通过为每个完成的时期打印一个点来显示训练进度
class PrintDot(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs):
    if epoch % 100 == 0: print('')
    print('.', end='')

EPOCHS = 1000
# 训练模型
history = model.fit(
  normed_train_data, train_labels,
  epochs=EPOCHS, validation_split = 0.2, verbose=0,
  callbacks=[PrintDot()])

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [MPG]')
  plt.plot(hist['epoch'], hist['mae'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mae'],
           label = 'Val Error')
  plt.ylim([0,5])
  plt.legend()

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Square Error [$MPG^2$]')
  plt.plot(hist['epoch'], hist['mse'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mse'],
           label = 'Val Error')
  plt.ylim([0,20])
  plt.legend()
  plt.show()

# plot_history(history)

# 用测试集评估模型
loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)

print("Testing set Mean Abs Error: {:5.2f} MPG".format(mae))

# 预测数据
test_predictions = model.predict(normed_test_data).flatten()

plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
plt.axis('equal')
plt.axis('square')
plt.xlim([0,plt.xlim()[1]])
plt.ylim([0,plt.ylim()[1]])
_ = plt.plot([-100, 100], [-100, 100])
plt.show()
# 误差分布
error = test_predictions - test_labels
plt.hist(error, bins = 25)
plt.xlabel("Prediction Error [MPG]")
_ = plt.ylabel("Count")
plt.show()
```

- 均方误差（MSE）是用于回归问题的常见损失函数（分类问题中使用不同的损失函数（交叉熵））。
- 类似的，用于回归的评估指标与分类不同。 常见的回归指标是平均绝对误差（MAE）（分类问题accuracy）。
- 当数字输入数据特征的值存在不同范围时，每个特征应独立缩放到相同范围。
- 如果训练数据不多，一种方法是选择隐藏层较少的小网络，以避免过度拟合。
- 早期停止是一种防止过度拟合的有效技术。

### 过拟合和欠拟合
[link->](https://tensorflow.google.cn/tutorials/keras/overfit_and_underfit#create_a_smaller_model)
此示例中的代码将使用 [tf.keras API](https://tensorflow.google.cn/api_docs/python/tf/keras)；
如需了解详情，请参阅 [TensorFlow Keras 指南](https://tensorflow.google.cn/guide/keras)

过拟合 不能很好地泛化到测试数据（或之前未见过的数据）的模型。  
欠拟合 模型(结构)不够强大、过于正则化，或者根本没有训练足够长的时间

处理过拟合的情况
1. 更多的训练数据
2. 数据量不多的情况用正则化计数处理
  - 限制存储信息(参数)的数量和类型
  - 只有少量的更突出的模式

本节介绍的正则化技术
- 权重正则化
- 丢弃

缩小模型防止过拟合 减少学习参数的容量（每层单元数和层数）  
参数越多，模型的“记忆容量”越大，样本和目标之间的字典式完美映射（就没有泛化）  
如果参数过少则无法轻松学习、拟合数据  
因此模型要找到合适的规模，拥有更强的预测力和压缩表示法

*目前并没有什么公式可用来确定合适的模型大小或架构，所以模型容量需要逐一尝试（从小到大）*

**策略**

* **权重正则化** *

奥卡姆剃刀定律：如果对于同一现象有两种解释，最可能正确的解释是“最简单”的解释，即做出最少量假设的解释。  
给定训练数据和一个网络架构，有多组权重值（多个模型）可以解释数据，而简单模型比复杂模型更不容易过拟合。

“简单模型”是一种参数值分布的熵较低的模型（或者具有较少参数的模型）
缓解过拟合，一种常见方法是限制网络的复杂性，具体方法是强制要求其权重仅采用较小的值，使权重值的分布更“规则”*(为什么较小的权重会分布更"规则")*,即**权重正则化**，通过向网络的损失函数添加与权重较大相关的代价来实现。（大概是权重越大损失会越大吧）
- **L1 正则化**，其中所添加的代价与**权重系数的绝对值**（即所谓的权重“L1 范数”）成正比。

- **L2 正则化**，其中所添加的代价与**权重系数值的平方**（即所谓的权重“L2 范数”）成正比。L2 正则化在神经网络领域也称为** *权重衰减* **。不要因为名称不同而感到困惑：从数学角度来讲，权重衰减与 L2 正则化完全相同。

tf.keras 中添加权重正则化将权重正则化项实例作为关键字参数传递给层。
```python
keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),
                  activation=tf.nn.relu, input_shape=(NUM_WORDS,)),
```
l2(0.001) 表示层的权重矩阵中的每个系数都会将 0.001 * weight_coefficient_value**2 添加到网络的总损失中。请注意，由于此惩罚仅在训练时添加，此网络在训练时的损失将远高于测试时。  
能够在相同的参数数量下获得更高的过拟合抵抗能力

* **添加丢弃层** *

在训练时丢弃层会随机的将输出向量设置为0， 丢弃率通常设置在0.2 和 0.5 之间  
在测试时，网络不会丢弃任何单元，而是将层的输出值按丢弃率的比例进行缩减，平衡测试时和训练时的实际活动单元数量

在 tf.keras 中将丢弃引入网络中（在普通的层之间引入丢弃层）

```python
keras.layers.Dropout(0.5),
```

```python
#python36
# https://tensorflow.google.cn/tutorials/keras/overfit_and_underfit

import tensorflow as tf
from tensorflow import keras

import numpy as np
import matplotlib.pyplot as plt

import time

print(tf.__version__) # 1.12.0-rc1

'''
{
  ts      //时间戳
  tsStr    //时间戳字符串
  offset  //相对起始时间偏移
  offsetStr//偏移字符串
  delta   //相对上一次的差
  deltaStr //delta字符串
  msg     //信息
}
'''
class Timestamp_c:
  def __init__(self,tsl,msg):
    t=time.time()*1000
    self.ts=t
    self.msg=msg
    if(len(tsl)==0):
      self.offset=0
      self.delta=0
    else:
      self.offset=t-tsl[0].ts
      self.delta=t-tsl[-1].ts
    self.tsStr=self.getTsStr()
    self.offsetStr=self.getOffsetStr()
    self.deltaStr=self.getDeltaStr()
  def formatMs(self,t):
    ms=t%1000
    s=t//1000
    m=s//60
    h=m//60
    s=s%60
    m=m%60
    return [h,m,s,ms]
  def getTsStr(self):
    tsl=time.localtime(self.ts/1000)
    return time.strftime("%Y-%m-%d %H:%M:%S",tsl)+" %f"%(self.ts%1000)
  def getOffsetStr(self):
    fm=self.formatMs(self.offset)
    return "%02d:%02d:%02d-%03d"%(fm[0],fm[1],fm[2],fm[3])
  def getDeltaStr(self):
    fm=self.formatMs(self.delta)
    return "%02d:%02d:%02d-%03d"%(fm[0],fm[1],fm[2],fm[3])

class TimestampList_c:
  def __init__(self,msg="start"):
    self.tsl=[]
    self.addts(msg)
  def addts(self,msg):
    self.tsl.append(Timestamp_c(self.tsl,msg))
  def string(self):
    s="start: "+self.tsl[0].tsStr+"\n"
    s+="| ts "+\
        "| offset "+\
        "| offsetStr "+\
        "| delta "+\
        "| deltaStr "+\
        "| msg |"+"\n"
    s+="| :--- "+\
        "| :--- "+\
        "| :--- "+\
        "| :--- "+\
        "| :--- "+\
        "| :--- |"+"\n"
    for val in self.tsl:
      s +="| %d | %d | %s | %d | %s | %s |\n"%(
          val.ts,
          val.offset,
          val.offsetStr,
          val.delta,
          val.deltaStr,
          val.msg,
      )
    return s
  def __str__(self):
    return self.string()

# tsl=TimestampList_c()
# time.sleep(0.2)
# tsl.addts("baseline_model")
# time.sleep(0.3)
# tsl.addts("baseline_model2")
# time.sleep(0.4)
# tsl.addts("baseline_model3")
# print(tsl)
# exit()

tsl=TimestampList_c()

NUM_WORDS = 10000

(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)
# 多热编码(have)
def multi_hot_sequences(sequences, dimension):
    # Create an all-zero matrix of shape (len(sequences), dimension)
    results = np.zeros((len(sequences), dimension))
    for i, word_indices in enumerate(sequences):
        results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s
    return results

train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)
test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)

# plt.plot(train_data[0])
# plt.show()

# 创建基本模型
tsl.addts("baseline_model")
baseline_model = keras.Sequential([
    # `input_shape` is only required here so that `.summary` works.
    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),
    keras.layers.Dense(16, activation=tf.nn.relu),
    keras.layers.Dense(1, activation=tf.nn.sigmoid)
])

baseline_model.compile(optimizer='adam',
                       loss='binary_crossentropy',
                       metrics=['accuracy', 'binary_crossentropy'])

baseline_model.summary()
# 基本模型训练
baseline_history = baseline_model.fit(train_data,
                                      train_labels,
                                      epochs=20,
                                      batch_size=512,
                                      validation_data=(test_data, test_labels),
                                      verbose=2)

# 创建小模型
tsl.addts("smaller_model")
smaller_model = keras.Sequential([
    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),
    keras.layers.Dense(4, activation=tf.nn.relu),
    keras.layers.Dense(1, activation=tf.nn.sigmoid)
])

smaller_model.compile(optimizer='adam',
                loss='binary_crossentropy',
                metrics=['accuracy', 'binary_crossentropy'])

smaller_model.summary()
# 小模型训练
smaller_history = smaller_model.fit(train_data,
                                    train_labels,
                                    epochs=20,
                                    batch_size=512,
                                    validation_data=(test_data, test_labels),
                                    verbose=2)

# 创建大模型
tsl.addts("bigger_model")
bigger_model = keras.models.Sequential([
    keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),
    keras.layers.Dense(512, activation=tf.nn.relu),
    keras.layers.Dense(1, activation=tf.nn.sigmoid)
])

bigger_model.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy','binary_crossentropy'])

bigger_model.summary()
# 大模型训练
bigger_history = bigger_model.fit(train_data, train_labels,
                                  epochs=20,
                                  batch_size=512,
                                  validation_data=(test_data, test_labels),
                                  verbose=2)

# 绘制图表
def plot_history(histories, key='binary_crossentropy'):
  tsl.addts("- plot_history start")
  plt.figure(figsize=(16,10))

  for name, history in histories:
    val = plt.plot(history.epoch, history.history['val_'+key],
                   '--', label=name.title()+' Val')
    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),
             label=name.title()+' Train')

  plt.xlabel('Epochs')
  plt.ylabel(key.replace('_',' ').title())
  plt.legend()

  plt.xlim([0,max(history.epoch)])
  tsl.addts("- plot_history show")

  plt.show()#
  tsl.addts("- plot_history exit")

plot_history([('baseline', baseline_history),
              ('smaller', smaller_history),
              ('bigger', bigger_history)])

# 策略 L2权重正则化
tsl.addts("l2_model")
l2_model = keras.models.Sequential([
    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),# 仅在训练时加入的权重正则化损失
                       activation=tf.nn.relu, input_shape=(NUM_WORDS,)),
    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),
                       activation=tf.nn.relu),
    keras.layers.Dense(1, activation=tf.nn.sigmoid)
])

l2_model.compile(optimizer='adam',
                 loss='binary_crossentropy',
                 metrics=['accuracy', 'binary_crossentropy'])

l2_model_history = l2_model.fit(train_data, train_labels,
                                epochs=20,
                                batch_size=512,
                                validation_data=(test_data, test_labels),
                                verbose=2)


plot_history([('baseline', baseline_history),
              ('l2', l2_model_history)])

# 策略 引入丢弃层
tsl.addts("dpt_model")
dpt_model = keras.models.Sequential([
    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(16, activation=tf.nn.relu),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1, activation=tf.nn.sigmoid)
])

dpt_model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy','binary_crossentropy'])

dpt_model_history = dpt_model.fit(train_data, train_labels,
                                  epochs=20,
                                  batch_size=512,
                                  validation_data=(test_data, test_labels),
                                  verbose=2)

plot_history([('baseline', baseline_history),
              ('dropout', dpt_model_history)])

tsl.addts("end")

print(tsl)
```

总结防止神经网络过拟合的常见方法：

- 获取更多训练数据。
- 降低网络容量。
- 添加权重正则化。
- 添加丢弃层。

- 数据增强和批次归一化。

### 保存和恢复模型
[link->](https://tensorflow.google.cn/tutorials/keras/save_and_load)

能够保存训练完成的模型和训练中的模型并加载后继续训练  
[安全使用Tensorflow](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md)  
[保存和恢复](https://tensorflow.google.cn/guide/eager#object-based_saving) [保存到 eager](https://tensorflow.google.cn/guide/eager#object-based_saving)

- checkpoint文件 格式化文件的集合
- Checkpoints文件 (cp.ckpt) 一个或多个包含模型权重的分片
- .index文件 索引 指示哪些权重存储在哪个分片中

- 回调保存权重
  - keras.callbacks.ModelCheckpoint配置获取回调
  - fit时设置callbacks
  - ModelCheckpoint的period参数设置保存频次 path = "{epoch:04d}.ckpt"区分保存文件
  - tf.train.latest_checkpoint(dir) 获取最后文件名
- model.save_weights手动保存权重
  - model.load_weights加载权重
- madel.save('xxx.h5') keras实验性方法保存模型和权重
  - 加载 keras.models.load_model
- tf方法保存模型、权重和编译好的tf优化器
  - 保存 tf.keras.experimental.export_saved_model(model, saved_model_path)
  - 加载 tf.keras.experimental.load_from_saved_model(saved_model_path)


```python
#!python36
# https://tensorflow.google.cn/tutorials/keras/save_and_load

from __future__ import absolute_import, division, print_function, unicode_literals

import os

import tensorflow as tf
from tensorflow import keras

# print(os.getcwd())
# print(__file__,os.path.realpath(__file__))
# print(os.path.sep)
# exit()
os.chdir(os.path.dirname(os.path.realpath(__file__)))
# print(os.getcwd())
# exit()
print(tf.version.VERSION) # 2.0.0
#获取示例数据集
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_labels = train_labels[:1000]
test_labels = test_labels[:1000]

train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0

def_epochs=10

# 定义一个简单的序列（sequential）模型
def create_model():
  model = tf.keras.models.Sequential([
    keras.layers.Dense(512, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
  ])

  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

  return model

# 创建一个基本的模型实例
model = create_model()

# 显示模型的结构
model.summary()

#在训练期间保存模型（checkpoints 形式）
def checkpoints():
  global model
  checkpoint_path = "training_1/cp.ckpt" # 使用固定的文件名 每一轮测试都会覆盖文件
  checkpoint_dir = os.path.dirname(checkpoint_path)

  # 创建一个保存模型权重的回调
  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                  save_weights_only=True, # 只保存权重
                                                  verbose=1) # 冗余
  
  # 使用新的回调训练模型
  model.fit(train_images, 
            train_labels,  
            epochs=def_epochs,# 10,
            validation_data=(test_images,test_labels),
            callbacks=[cp_callback])  # 通过回调训练

  # 这可能会生成与保存优化程序状态相关的警告。
  # 这些警告（以及整个笔记本中的类似警告）是防止过时使用，可以忽略。

  # 创建一个基本模型实例
  model = create_model()# 一个新的模型实例 覆盖了之前的model

  # 评估模型
  # (机会水平（chance levels)正确率的模型)
  loss, acc = model.evaluate(test_images,  test_labels, verbose=2)
  print("Untrained model, accuracy: {:5.2f}%".format(100*acc))

  # 加载权重并重新评估
  # 加载权重
  model.load_weights(checkpoint_path)

  # 重新评估模型
  loss,acc = model.evaluate(test_images,  test_labels, verbose=2)
  print("Restored model, accuracy: {:5.2f}%".format(100*acc))


######################################################################
# checkpoint 回调选项(- 回调频率)

def checkpoints_para():
  # 在文件名中包含 epoch (使用 `str.format`)
  checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
  checkpoint_dir = os.path.dirname(checkpoint_path)

  # 创建一个回调，每 5 个 epochs 保存模型的权重
  cp_callback = tf.keras.callbacks.ModelCheckpoint(
      filepath=checkpoint_path, 
      verbose=1, 
      save_weights_only=True,
      period=5)

  # 创建一个新的模型实例
  model = create_model()

  # 使用 `checkpoint_path` 格式保存权重
  model.save_weights(checkpoint_path.format(epoch=0)) # 手动保存初始模型

  # 使用新的回调*训练*模型
  model.fit(train_images, 
                train_labels,
                epochs=def_epochs,# 50, 
                callbacks=[cp_callback],
                validation_data=(test_images,test_labels),
                verbose=0)

  latest = tf.train.latest_checkpoint(checkpoint_dir)
  latest # 貌似 tf v2.0 默认仅保存最近的5个 checkpoint 

  # 创建一个新的模型实例
  model = create_model()

  # 加载以前保存的权重
  model.load_weights(latest)

  # 重新评估模型
  loss, acc = model.evaluate(test_images,  test_labels, verbose=2)
  print("Restored model, accuracy: {:5.2f}%".format(100*acc))

################################################################
# 手动保存
# 保存权重
def save_weights():
  global model
  model.save_weights('./checkpoints/my_checkpoint')

  # 创建模型实例
  model = create_model()

  # Restore the weights
  model.load_weights('./checkpoints/my_checkpoint')

  # Evaluate the model
  loss,acc = model.evaluate(test_images,  test_labels, verbose=2)
  print("Restored model, accuracy: {:5.2f}%".format(100*acc))

################################################################
# 保存整个模型
# 模型和优化器可以保存到包含其状态（权重和变量）和模型参数的文件中

def keras_save_model():
  # 用Keras 实验性的model.save保存 HDF5标准
  # 包含权重 模型配置(结构) 优化器配置
  # 不包含tf优化器 需要在加载后重新编译模型
  # 创建一个新的模型实例
  model = create_model()

  # 训练模型
  model.fit(train_images, train_labels, epochs=def_epochs)# 5)

  # 将整个模型保存为HDF5文件
  model.save('my_model.h5')


  # 重新创建完全相同的模型，包括其权重和优化程序
  new_model = keras.models.load_model('my_model.h5')

  # 显示网络结构
  new_model.summary()

  # 验证准确率
  loss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)
  print("Restored model, accuracy: {:5.2f}%".format(100*acc))


def tf_save_model():
  # 用tf.save_model保存
  model = create_model()

  model.fit(train_images, train_labels, epochs=def_epochs)# 5)

  import time
  # saved_model_path = "./saved_models/{}".format(int(time.time()))
  saved_model_path = "./saved_models/tf_model"

  tf.keras.experimental.export_saved_model(model, saved_model_path)
  saved_model_path

  # 重新加载
  new_model = tf.keras.experimental.load_from_saved_model(saved_model_path)

  # 显示网络结构
  new_model.summary()

  #预测数据
  model.predict(test_images).shape

  # 必须在评估之前编译模型。
  # 如果仅部署已保存的模型，则不需要此步骤。

  # 加载的模型不不包含评估器

  new_model.compile(optimizer=model.optimizer,  # 保留已加载的优化程序
                    loss='sparse_categorical_crossentropy', #这没有保存吗
                    metrics=['accuracy'])

  # 评估已恢复的模型
  loss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)
  print("Restored model, accuracy: {:5.2f}%".format(100*acc))


checkpoints()
checkpoints_para()
save_weights()
keras_save_model()
keras_save_model()
tf_save_model()
```